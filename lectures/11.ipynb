{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "953ad978",
   "metadata": {
    "deletable": false
   },
   "source": [
    "# [Introduction to Data Science: A Comp-Math-Stat Approach](http://datascience-intro.github.io/1MS041-2021/)    \n",
    "## 1MS041, 2021 \n",
    "&copy;2021 Raazesh Sainudiin, Benny Avelin. [Attribution 4.0 International     (CC BY 4.0)](https://creativecommons.org/licenses/by/4.0/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 11. Non-parametric Estimation and Testing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Topics\n",
    "\n",
    "- Nonparametric Estimation\n",
    "- Glivenko-Cantelli Theorem\n",
    "- Dvoretsky-Kiefer-Wolfowitz Inequality\n",
    "- Plug-in Estimator\n",
    "- Nonparametric Confidence Intervals via Bootstrap\n",
    "- Nonparametric Hypothesis Testing \n",
    "- Permutation Testing\n",
    "- Permutation Testing with Shells Data\n",
    "\n",
    "  \n",
    "\n",
    "## Inference and Estimation: The Big Picture\n",
    "\n",
    "The Big Picture is about inference and estimation, and especially inference and estimation problems where computational techniques are helpful. \n",
    "\n",
    "<table border=\"1\" cellspacing=\"2\" cellpadding=\"2\" align=\"center\">\n",
    "<tbody>\n",
    "<tr>\n",
    "<td style=\"background-color: #ccccff;\" align=\"center\">&nbsp;</td>\n",
    "<td style=\"background-color: #ccccff;\" align=\"center\"><strong>Point estimation</strong></td>\n",
    "<td style=\"background-color: #ccccff;\" align=\"center\"><strong>Set estimation</strong></td>\n",
    "<td style=\"background-color: #ccccff;\" align=\"center\"><strong>Hypothesis Testing</strong></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"background-color: #ccccff;\">\n",
    "<p><strong>Parametric</strong></p>\n",
    "<p>&nbsp;</p>\n",
    "</td>\n",
    "<td style=\"background-color: #ccccff;\" align=\"center\">\n",
    "<p>MLE of finitely many parameters<br /><span style=\"color: #3366ff;\"><em>done</em></span></p>\n",
    "</td>\n",
    "<td style=\"background-color: #ccccff;\" align=\"center\">\n",
    "<p>Asymptotically Normal Confidence Intervals<br /><span style=\"color: #3366ff;\"><em>done</em></span></p>\n",
    "</td>\n",
    "<td style=\"background-color: #ccccff;\" align=\"center\">\n",
    "<p>Wald Test from Confidence Interval<br /><span style=\"color: #3366ff;\"><em>done</em></span></p>\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"background-color: #ccccff;\">\n",
    "<p><strong>Non-parametric</strong><br /> (infinite-dimensional parameter space)</p>\n",
    "</td>\n",
    "<td style=\"background-color: #ccccff;\" align=\"center\"><strong><em><span style=\"color: #3366ff;\">about to see ... </span></em></strong></td>\n",
    "<td style=\"background-color: #ccccff;\" align=\"center\"><strong><em><span style=\"color: #3366ff;\">about to see ... </span></em></strong></td>\n",
    "<td style=\"background-color: #ccccff;\" align=\"center\"><strong><em><span style=\"color: #3366ff;\">about to see ... </span></em></strong></td>\n",
    "</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "So far we have seen parametric models, for example\n",
    "\n",
    "- $X_1, X_2, \\ldots, X_n \\overset{IID}{\\sim} Bernoulli (\\theta)$, $\\theta \\in [0,1]$\n",
    "- $X_1, X_2, \\ldots, X_n \\overset{IID}{\\sim} Exponential (\\lambda)$, $\\lambda \\in (0,\\infty)$\n",
    "- $X_1, X_2, \\ldots, X_n \\overset{IID}{\\sim} Normal(\\mu^*, \\sigma)$, $\\mu \\in \\mathbb{R}$, $\\sigma \\in (0,\\infty)$\n",
    "\n",
    "In all these cases **the parameter space** (the space within which the parameter(s) can take values) is **finite dimensional**:\n",
    "\n",
    "- for the $Bernoulli$, $\\theta \\in [0,1] \\subseteq \\mathbb{R}^1$\n",
    "- for the $Exponential$, $\\lambda \\in (0, \\infty) \\subseteq \\mathbb{R}^1$\n",
    "- for the $Normal$, $\\mu \\in \\mathbb{R}^1$, $\\sigma \\in (0,\\infty) \\subseteq \\mathbb{R}^1$, so $(\\mu, \\sigma) \\subseteq \\mathbb{R}^2$\n",
    "\n",
    "For parametric experiments, we can use the maximum likelihood principle and estimate the parameters using the **Maximum Likelihood Estimator (MLE)**, for instance. \n",
    "\n",
    "## Non-parametric estimation\n",
    "\n",
    "Suppose we don't know what the distribution function (DF) is?  We are not trying to estimate some fixed but unknown parameter $\\theta^*$ for some RV we are assuming to be $Bernoulli(\\theta^*)$, we are trying to estimate the DF itself.  In real life, data does not come neatly labeled \"I am a realisation of a $Bernoulli$ RV\", or \"I am a realisation of an $Exponential$ RV\": an important part of inference and estimation is to make inferences about the DF itself from our observations.   \n",
    "\n",
    "### Observations from some unknown process\n",
    "<img src=\"images/unknownProcessTimesAnim.gif\" width=400>\n",
    "\n",
    "Consider the following non-parametric product experiment:\n",
    "\n",
    "$$X_1, X_2, \\ldots, X_n\\ \\overset{IID}{\\sim} F^* \\in \\{\\text{all DFs}\\}$$\n",
    "\n",
    "We want to produce a point estimate for $F^*$, which is a allowed to be any DF (\"lives in the set of all DFs\"), i.e., $F^* \\in \\{\\text{all DFs}\\}$\n",
    "\n",
    "Crucially, $\\{\\text{all DFs}\\}$, i.e., the set of all distribution functions over $\\mathbb{R}$ is infinite dimensional.\n",
    "\n",
    "<img src=\"images/TwoDFs.png\" width=400>\n",
    "\n",
    "We have already seen an estimate, made using the data, of a distribution function:  the empirical or data-based distribution function (or empirical cumulative distribution function). This can be formalized as the following process of adding indicator functions of the half-lines beginning at the data points $[X_1,+\\infty),[X_2,+\\infty),\\ldots,[X_n,+\\infty)$:\n",
    "\n",
    "$$\\widehat{F}_n (x) = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}_{[X_i,+\\infty)}(x)$$\n",
    "\n",
    "\n",
    "where,\n",
    "\n",
    "$$\\mathbf{1}_{[X_i,+\\infty)}(x) := \\begin{cases} & 1 \\quad \\text{ if } X_i \\leq x \\\\ & 0 \\quad \\text{ if }X_i > x \\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "First let us evaluate a set of functions that will help us conceptualize faster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def makeEMFHidden(myDataList):\n",
    "    '''Make an empirical mass function from a data list.\n",
    "    \n",
    "    Param myDataList, list of data to make emf from.\n",
    "    Return list of tuples comprising (data value, relative frequency) ordered by data value.'''\n",
    "    \n",
    "    sortedUniqueValues = sorted(list(set(myDataList)))\n",
    "    freqs = [myDataList.count(i) for i in sortedUniqueValues]\n",
    "    relFreqs = [ZZ(fr)/len(myDataList) for fr in freqs] # use a list comprehension\n",
    "    \n",
    "    return list(zip(sortedUniqueValues, relFreqs))\n",
    "    \n",
    "\n",
    "from pylab import array\n",
    "\n",
    "def makeEDFHidden(myDataList, offset=0):\n",
    "    '''Make an empirical distribution function from a data list.\n",
    "    \n",
    "    Param myDataList, list of data to make ecdf from.\n",
    "    Param offset is an offset to adjust the edf by, used for doing confidence bands.\n",
    "    Return list of tuples comprising (data value, cumulative relative frequency) ordered by data value.'''\n",
    "    \n",
    "    sortedUniqueValues = sorted(list(set(myDataList)))\n",
    "    freqs = [myDataList.count(i) for i in sortedUniqueValues]\n",
    "    from pylab import cumsum\n",
    "    cumFreqs = list(cumsum(freqs)) #\n",
    "    cumRelFreqs = [ZZ(i)/len(myDataList) for i in cumFreqs] # get cumulative relative frequencies as rationals\n",
    "    if offset > 0: # an upper band\n",
    "        cumRelFreqs = [min(i ,1) for i in cumRelFreqs] # use a list comprehension\n",
    "    if offset < 0: # a lower band\n",
    "        cumRelFreqs = [max(i, 0) for i in cumFreqs] # use a list comprehension\n",
    "    return list(zip(sortedUniqueValues, cumRelFreqs))\n",
    "    \n",
    "# EPMF plot\n",
    "def epmfPlot(samples):\n",
    "    '''Returns an empirical probability mass function plot from samples data.'''\n",
    "    \n",
    "    epmf_pairs = makeEMFHidden(samples)\n",
    "    epmf = point(epmf_pairs, rgbcolor = \"blue\", pointsize=\"20\")\n",
    "    for k in epmf_pairs:    # for each tuple in the list\n",
    "        kkey, kheight = k     # unpack tuple\n",
    "        epmf += line([(kkey, 0),(kkey, kheight)], rgbcolor=\"blue\", linestyle=\":\")\n",
    "    # padding\n",
    "    epmf += point((0,1), rgbcolor=\"black\", pointsize=\"0\")\n",
    "    return epmf\n",
    "    \n",
    "\n",
    "# ECDF plot\n",
    "def ecdfPlot(samples):\n",
    "    '''Returns an empirical probability mass function plot from samples data.'''\n",
    "    ecdf_pairs = makeEDFHidden(samples)\n",
    "    ecdf = point(ecdf_pairs, rgbcolor = \"red\", faceted = false, pointsize=\"20\")\n",
    "    for k in range(len(ecdf_pairs)):\n",
    "        x, kheight = ecdf_pairs[k]     # unpack tuple\n",
    "        previous_x = 0\n",
    "        previous_height = 0\n",
    "        if k > 0:\n",
    "            previous_x, previous_height = ecdf_pairs[k-1] # unpack previous tuple\n",
    "        ecdf += line([(previous_x, previous_height),(x, previous_height)], rgbcolor=\"grey\")\n",
    "        ecdf += points((x, previous_height),rgbcolor = \"white\", faceted = true, pointsize=\"20\")\n",
    "        ecdf += line([(x, previous_height),(x, kheight)], rgbcolor=\"grey\", linestyle=\":\")\n",
    "    # padding\n",
    "    ecdf += line([(ecdf_pairs[0][0]-0.2, 0),(ecdf_pairs[0][0], 0)], rgbcolor=\"grey\")\n",
    "    max_index = len(ecdf_pairs)-1\n",
    "    ecdf += line([(ecdf_pairs[max_index][0], ecdf_pairs[max_index][1]),(ecdf_pairs[max_index][0]+0.2, ecdf_pairs[max_index][1])],rgbcolor=\"grey\")\n",
    "    return ecdf\n",
    "    \n",
    "def calcEpsilon(alphaE, nE):\n",
    "    '''Return confidence band epsilon calculated from parameters alphaE > 0 and nE > 0.'''\n",
    "    \n",
    "    return sqrt(1/(2*nE)*log(2/alphaE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Let us continue with the concepts\n",
    "\n",
    "We can remind ourselves of this for a small sample of $de\\,Moivre(k=5)$ RVs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 1, 1, 2, 3, 2, 4, 4, 4, 1, 4, 3, 4, 5, 1, 1, 5, 5, 4]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deMs=[randint(1,5) for i in range(20)]  # randint can be used to uniformly sample integers in a specified range\n",
    "deMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 3/10), (2, 2/5), (3, 11/20), (4, 17/20), (5, 1)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortedUniqueValues = sorted(list(set(deMs)))\n",
    "freqs = [deMs.count(i) for i in sortedUniqueValues]\n",
    "from pylab import cumsum\n",
    "cumFreqs = list(cumsum(freqs)) #\n",
    "cumRelFreqs = [ZZ(i)/len(deMs) for i in cumFreqs] # get cumulative relative frequencies as rationals\n",
    "list(zip(sortedUniqueValues, cumRelFreqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "a03786f6f5ef501d05358b5d4d68abdf01514d5b",
      "text/plain": [
       "Graphics object consisting of 18 graphics primitives"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show(ecdfPlot(deMs), figsize=[6,3]) # use hidden ecdfPlot function to plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can use the empirical cumulative distribution function $\\widehat{F}_n$ for our non-parametric estimate because this kind of estimation is possible in infinite-dimensional contexts due to the following two theorems:\n",
    "\n",
    "- Glivenko-Cantelli Theorem (*Fundamental Theorem of Statistics*)\n",
    "- Dvoretsky-Kiefer-Wolfowitz (DKW) Inequality\n",
    "\n",
    "## Glivenko-Cantelli Theorem\n",
    "\n",
    "Let $X_1, X_2, \\ldots, X_n \\overset{IID}{\\sim} F^* \\in \\{\\text{all DFs}\\}$\n",
    "\n",
    "and the empirical distribution function (EDF) is $\\widehat{F}_n(x) := \\displaystyle\\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}_{[X_i,+\\infty)}(x)$, then\n",
    "\n",
    "$$\\sup_x { | \\widehat{F}_n(x) - F^*(x) | } \\overset{P}{\\rightarrow} 0$$\n",
    "\n",
    "Remember that the EDF is a statistic of the data, a statistic is an RV, and (from our work the convergence of random variables), $\\overset{P}{\\rightarrow}$ means \"converges in probability\".  The proof is beyond the scope of this course, but we can gain an appreciation of what it means by looking at what happens to the ECDF for $n$ simulations from:\n",
    "\n",
    "- $de\\,Moivre(1/5,1/5,1/5,1/5,1/5)$ and  \n",
    "- $Uniform(0,1)$ as $n$ increases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "940940c06954448aa6dc759be58e1593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Interactive function <function _ at 0x7f3f0439bae8> with 1 widget\n",
       "  n: SelectionSlider(description='n', index=…"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@interact\n",
    "def _(n=(10,(0..200))):\n",
    "    '''Interactive function to plot ecdf for obs from de Moirve (5).'''\n",
    "    if (n > 0):\n",
    "        us = [randint(1,5) for i in range(n)]\n",
    "        p=ecdfPlot(us) # use hidden ecdfPlot function to plot\n",
    "        #p+=line([(-0.2,0),(0,0),(1,1),(1.2,1)],linestyle=':')\n",
    "        p.show(figsize=[8,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86da16a6418640eda97a4e3e4d216fbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Interactive function <function _ at 0x7f3f04348b70> with 1 widget\n",
       "  n: SelectionSlider(description='n', index=…"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@interact\n",
    "def _(n=(10,(0..200))):\n",
    "    '''Interactive function to plot ecdf for obs from Uniform(0,1).'''\n",
    "    if (n > 0):\n",
    "        us = [random() for i in range(n)]\n",
    "        p=ecdfPlot(us) # use hidden ecdfPlot function to plot\n",
    "        p+=line([(-0.2,0),(0,0),(1,1),(1.2,1)],linestyle='-')\n",
    "        p.show(figsize=[3,3],aspect_ratio=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "It is clear, that as $n$ increases, the ECDF $\\widehat{F}_n$ gets closer and closer to the true DF $F^*$,   $\\displaystyle\\sup_x { | \\widehat{F}_n(x) - F^*(x) | } \\overset{P}{\\rightarrow} 0$.\n",
    "\n",
    "This will hold no matter what the (possibly unknown) $F^*$ is.  Thus, $\\widehat{F}_n$ is a point estimate of $F^*$.\n",
    "\n",
    "We need to add the DKW Inequality be able to get confidence sets or a 'confidence band' that traps $F^*$ with high probability.\n",
    "\n",
    "## Dvoretsky-Kiefer-Wolfowitz (DKW) Inequality\n",
    "\n",
    "Let $X_1, X_2, \\ldots, X_n \\overset{IID}{\\sim} F^* \\in \\{\\text{all DFs}\\}$\n",
    "\n",
    "and the empirical distribution function (EDF) is $\\widehat{F}_n(x) := \\displaystyle\\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}_{[X_i,+\\infty)}(x)$,\n",
    "\n",
    "then, for any $\\varepsilon > 0$,\n",
    "\n",
    "$$P\\left( \\sup_x { | \\widehat{F}_n(x) - F^*(x) | > \\varepsilon  }\\right) \\leq 2 \\exp(-2n\\varepsilon^2) $$\n",
    "\n",
    "We can use this inequality to get a $1-\\alpha$ confidence band $C_n(x) := \\left[\\underline{C}_n(x), \\overline{C}_n(x)\\right]$ about our point estimate $\\widehat{F}_n$ of our possibly unknown $F^*$ such that the $F^*$ is 'trapped' by the band with probability at least $1-\\varepsilon$.\n",
    "\n",
    "$$\\begin{aligned} \\underline{C}_{\\, n}(x) &=& \\max \\{ \\widehat{F}_n(x)-\\varepsilon_n, 0 \\}, \\notag \\\\ \\overline{C}_{\\, n}(x) &=& \\min \\{ \\widehat{F}_n(x)+\\varepsilon_n, 1 \\}, \\notag \\\\ \\varepsilon_n &=& \\sqrt{ \\frac{1}{2n} \\log \\left( \\frac{2}{\\alpha}\\right)} \\\\ \\end{aligned}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$P\\left(\\underline{C}_n(x) \\leq F^*(x) \\leq \\overline{C}_n(x)\\right) \\geq 1-\\alpha$$\n",
    "\n",
    " \n",
    "\n",
    "## YouTry in class\n",
    "\n",
    "Try this out for a simple sample from the $Uniform(0,1)$, which you can generate using random.   First we  will just make the point estimate for $F^*$, the EDF $\\widehat{F}_n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7080534092577805, 0.9526810488576604, 0.3130366899289021, 0.811206154341072, 0.16865419994024078, 0.8307760107371247, 0.38034147148957487, 0.8108010578684278, 0.9751294259315408, 0.5480858527701734]\n"
     ]
    }
   ],
   "source": [
    "n=10\n",
    "uniformSample = [random() for i in range(n)]\n",
    "print(uniformSample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In one of the assessments, you did a question that took you through the steps for getting the list of points that you would plot for an empirical  distribution function (EDF).  We will do exactly the same thing here.\n",
    "\n",
    "First we find the unique values in the sample, in order from smallest to largest, and get the frequency with which each unique value occurs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.16865419994024078, 0.3130366899289021, 0.38034147148957487, 0.5480858527701734, 0.7080534092577805, 0.8108010578684278, 0.811206154341072, 0.8307760107371247, 0.9526810488576604, 0.9751294259315408]\n"
     ]
    }
   ],
   "source": [
    "sortedUniqueValuesUniform = sorted(list(set(uniformSample)))\n",
    "print(sortedUniqueValuesUniform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqsUniform = [uniformSample.count(i) for i in sortedUniqueValuesUniform]\n",
    "freqsUniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Then we accumulate the frequences to get the cumulative frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pylab import cumsum\n",
    "cumFreqsUniform = list(cumsum(freqsUniform)) # accumulate\n",
    "cumFreqsUniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "And the the relative cumlative frequencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1/10, 1/5, 3/10, 2/5, 1/2, 3/5, 7/10, 4/5, 9/10, 1]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cumulative rel freqs as rationals\n",
    "cumRelFreqsUniform = [ZZ(i)/len(uniformSample) for i in cumFreqsUniform] \n",
    "cumRelFreqsUniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "And finally zip these up with the sorted unique values to get a list of points we can plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.16865419994024078, 1/10),\n",
       " (0.3130366899289021, 1/5),\n",
       " (0.38034147148957487, 3/10),\n",
       " (0.5480858527701734, 2/5),\n",
       " (0.7080534092577805, 1/2),\n",
       " (0.8108010578684278, 3/5),\n",
       " (0.811206154341072, 7/10),\n",
       " (0.8307760107371247, 4/5),\n",
       " (0.9526810488576604, 9/10),\n",
       " (0.9751294259315408, 1)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecdfPointsUniform = list(zip(sortedUniqueValuesUniform, cumRelFreqsUniform))\n",
    "ecdfPointsUniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Here is a function that you can just use to do a ECDF plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ECDF plot given a list of points to plot\n",
    "def ecdfPointsPlot(listOfPoints, colour='grey', lines_only=False):\n",
    "    '''Returns an empirical probability mass function plot from a list of points to plot.\n",
    "    \n",
    "    Param listOfPoints is the list of points to plot.\n",
    "    Param colour is used for plotting the lines, defaulting to grey.\n",
    "    Param lines_only controls wether only lines are plotted (true) or points are added (false, the default value).\n",
    "    Returns an ecdf plot graphic.'''\n",
    "    \n",
    "    ecdfP = point((0,0), pointsize=\"0\")\n",
    "    if not lines_only: ecdfP = point(listOfPoints, rgbcolor = \"red\", faceted = false, pointsize=\"20\")\n",
    "    for k in range(len(listOfPoints)):\n",
    "        x, kheight = listOfPoints[k]     # unpack tuple\n",
    "        previous_x = 0\n",
    "        previous_height = 0\n",
    "        if k > 0:\n",
    "            previous_x, previous_height = listOfPoints[k-1] # unpack previous tuple\n",
    "        ecdfP += line([(previous_x, previous_height),(x, previous_height)], rgbcolor=colour)\n",
    "        ecdfP += line([(x, previous_height),(x, kheight)], rgbcolor=colour, linestyle=\":\")\n",
    "        if not lines_only: \n",
    "            ecdfP += points((x, previous_height),rgbcolor = \"black\", faceted = true, pointsize=\"20\")\n",
    "            # padding\n",
    "    max_index = len(listOfPoints)-1\n",
    "    ecdfP += line([(listOfPoints[0][0]-0.2, 0),(listOfPoints[0][0], 0)], rgbcolor=colour)\n",
    "    ecdfP += line([(listOfPoints[max_index][0], listOfPoints[max_index][1]),(listOfPoints[max_index][0]+0.2,\\\n",
    "                                                                listOfPoints[max_index][1])],rgbcolor=colour)\n",
    "    return ecdfP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This makes the plot of the $\\widehat{F}_{10}$, the point estimate for $F^*$ for these $n=10$ simulated samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "a0dd342ce4a5aabf2ff46d896e18e08fb30ca771",
      "text/plain": [
       "Graphics object consisting of 33 graphics primitives"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show(ecdfPointsPlot(ecdfPointsUniform), figsize=[6,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "What about adding those confidence bands?  You will do essentially the same thing, but adjusting for the required $\\varepsilon$.  First we need to decide on an $\\alpha$ and calculate the $\\varepsilon$ corresponding to this alpha.  Here is some of our code to calculate the $\\varepsilon$ corresponding to $\\alpha=0.05$ (95% confidence bands), using a hidden function calcEpsilon:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.429469408346738"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha = 0.05\n",
    "epsilon = calcEpsilon(alpha, n)\n",
    "epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "See if you can write your own code to do this calculation, $\\varepsilon_n = \\sqrt{ \\frac{1}{2n} \\log \\left( \\frac{2}{\\alpha}\\right)}$. For completeness, do the whole thing:assign the value 0.05 to a variable named alpha, and then use this and the variable called n that we have already declared to calculate a value for $\\varepsilon$.  Call the variable to which you assign the value for $\\varepsilon$  epsilon so that it replaces the value we calculated in the cell above (you should get the same value as us!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now we need to use this to adjust the EDF plot.  In the two cells below we first of all do the adjustment for $\\underline{C}_{\\,n}(x) =\\max \\{ \\widehat{F}_n(x)-\\varepsilon_n, 0 \\}$, and then use zip again to get the points to actually plot for the lower boundary of the 95% confidence band.\n",
    "\n",
    "Now we need to use this to adjust the EDF plot. In the two cells below we first of all do the adjustment for $\\overline{C}_{\\,n}(x) =\\min \\{ \\widehat{F}_n(x)+\\varepsilon_n, 1 \\}$, and then use zip again to get the points to actually plot for the lower boundary of the 95% confidence band."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0.0705305916532624, 0.170530591653262, 0.270530591653262, 0.370530591653262, 0.470530591653262, 0.570530591653262]\n"
     ]
    }
   ],
   "source": [
    "# heights for the lower band\n",
    "cumRelFreqsUniformLower = [max(crf - epsilon, 0) for crf in cumRelFreqsUniform] \n",
    "print(cumRelFreqsUniformLower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.16865419994024078, 0),\n",
       " (0.3130366899289021, 0),\n",
       " (0.38034147148957487, 0),\n",
       " (0.5480858527701734, 0),\n",
       " (0.7080534092577805, 0.0705305916532624),\n",
       " (0.8108010578684278, 0.170530591653262),\n",
       " (0.811206154341072, 0.270530591653262),\n",
       " (0.8307760107371247, 0.370530591653262),\n",
       " (0.9526810488576604, 0.470530591653262),\n",
       " (0.9751294259315408, 0.570530591653262)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecdfPointsUniformLower = list(zip(sortedUniqueValuesUniform, cumRelFreqsUniformLower))\n",
    "ecdfPointsUniformLower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We carefully gave our `ecdfPointsPlo`t function the flexibility to be able to plot bands, by having a colour parameter (which defaults to 'grey') and a `lines_only` parameter (which defaults to `false`).  Here we can plot the lower bound of the confidence interval by adding `ecdfPointsPlot(ecdfPointsUniformLower, colour='green', lines_only=true)` to the previous plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "5aeaa72aed5ead6ee3883b7d455e5cc8e5dc7496",
      "text/plain": [
       "Graphics object consisting of 56 graphics primitives"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pointEstimate = ecdfPointsPlot(ecdfPointsUniform)\n",
    "lowerBound = ecdfPointsPlot(ecdfPointsUniformLower, colour='green', lines_only=true)\n",
    "show(pointEstimate + lowerBound, figsize=[6,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### YouTry \n",
    "You try writing the code to create the list of points needed for plotting the upper band $\\overline{C}_{\\,n}(x) =\\min \\{ \\widehat{F}_n(x)+\\varepsilon_n, 1 \\}$.  You will need to first of all get the upper heights (call them say `cumRelFreqsUniformUpper`) and then `zip` them up with the `sortedUniqueValuesUniform` to get the points to plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# heights for the upper band\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Once you have got done this you can add them to the plot by altering the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "5aeaa72aed5ead6ee3883b7d455e5cc8e5dc7496",
      "text/plain": [
       "Graphics object consisting of 56 graphics primitives"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pointEstimate = ecdfPointsPlot(ecdfPointsUniform)\n",
    "lowerBound = ecdfPointsPlot(ecdfPointsUniformLower,colour='green', lines_only=true)\n",
    "show(pointEstimate + lowerBound, figsize=[6,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "(end of YouTry)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "If we are doing lots of collections of EDF points we may as well define a function to do it, rather than repeating the same code again and again.  We use an offset parameter to give us the flexibility to use this to make points for confidence bands as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def makeEDFPoints(myDataList, offset=0):\n",
    "    '''Make a list empirical distribution plotting points from from a data list.\n",
    "    \n",
    "    Param myDataList, list of data to make ecdf from.\n",
    "    Param offset is an offset to adjust the edf by, used for doing confidence bands.\n",
    "    Return list of tuples comprising (data value, cumulative relative frequency(with offset)) \n",
    "    ordered by data value.'''\n",
    "    \n",
    "    sortedUniqueValues = sorted(list(set(myDataList)))\n",
    "    freqs = [myDataList.count(i) for i in sortedUniqueValues]\n",
    "    from pylab import cumsum\n",
    "    cumFreqs = list(cumsum(freqs)) \n",
    "    cumRelFreqs = [ZZ(i)/len(myDataList) for i in cumFreqs] # get cumulative relative frequencies as rationals\n",
    "    if offset > 0: # an upper band\n",
    "        cumRelFreqs = [min(i+offset ,1) for i in cumRelFreqs]\n",
    "    if offset < 0: # a lower band\n",
    "        cumRelFreqs = [max(i+offset, 0) for i in cumRelFreqs] \n",
    "    return list(zip(sortedUniqueValues, cumRelFreqs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### NZ EartQuakes\n",
    "\n",
    "Now we will try looking at the Earthquakes data we have used before to get a confidence band around an EDF for that.  We start by bringing in the data and the function we wrote earlier to parse that data.\n",
    "\n",
    "First check if you have already `unzip`-ped `data/earthquakes.csv.zip` file by dropping in shell via `%%sh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/earthquakes.csv\n",
      "data/earthquakes.csv.zip\n",
      "data/earthquakes.tgz\n",
      "data/earthquakes_small.csv\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "ls data/earthquakes* # the '*' just lists all files that start with earthquakes in data/ directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 user user 4085555 Feb 12  2021 earthquakes.csv\n",
      "-rw-r--r-- 1 user user 1344114 Feb 12  2021 earthquakes.csv.zip\n",
      "-rw-r--r-- 1 user user 1344959 Feb 12  2021 earthquakes.tgz\n",
      "-rw-r--r-- 1 user user   77786 Feb 12  2021 earthquakes_small.csv\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "# only do this once! So, you don't need to do this step if you see earthquakes.csv file above\n",
    "cd data\n",
    "# windows and mac users should first try to unzip by uncommenting next line\n",
    "# unzip earthquakes.csv.zip\n",
    "## if unzip is not found try tar by uncommenting next line and commenting the above line\n",
    "#tar zxvf earthquakes.tgz\n",
    "ls -al earthquakes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21015"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getLonLatMagDepTimes(NZEQCsvFileName):\n",
    "    '''returns longitude, latitude, magnitude, depth and the origin time as unix time\n",
    "    for each observed earthquake in the csv filr named NZEQCsvFileName'''\n",
    "    from datetime import datetime\n",
    "    import time\n",
    "    from dateutil.parser import parse\n",
    "    import numpy as np\n",
    "    \n",
    "    with open(NZEQCsvFileName) as f:\n",
    "        reader = f.read() \n",
    "        dataList = reader.split('\\n')\n",
    "        \n",
    "    myDataAccumulatorList =[]\n",
    "    for data in dataList[1:-1]:\n",
    "        dataRow = data.split(',')\n",
    "        myTimeString = dataRow[2] # origintime\n",
    "        # let's also grab longitude, latitude, magnitude, depth\n",
    "        myDataString = [dataRow[4],dataRow[5],dataRow[6],dataRow[7]]\n",
    "        try: \n",
    "            myTypedTime = time.mktime(parse(myTimeString).timetuple())\n",
    "            myFloatData = [float(x) for x in myDataString]\n",
    "            myFloatData.append(myTypedTime) # append the processed timestamp\n",
    "            myDataAccumulatorList.append(myFloatData)\n",
    "        except TypeError as e: # error handling for type incompatibilities\n",
    "            print ('Error:  Error is ', e)\n",
    "    #return np.array(myDataAccumulatorList)\n",
    "    return myDataAccumulatorList\n",
    "\n",
    "myProcessedList = getLonLatMagDepTimes('data/earthquakes.csv')\n",
    "\n",
    "def interQuakeTimes(quakeTimes):\n",
    "    '''Return a list inter-earthquake times in seconds from earthquake origin times\n",
    "    Date and time elements are expected to be in the 5th column of the array\n",
    "    Return a list of inter-quake times in seconds. NEEDS sorted quakeTimes Data'''\n",
    "    import numpy as np\n",
    "    retList = []\n",
    "    if len(quakeTimes) > 1:\n",
    "        retList = [quakeTimes[i]-quakeTimes[i-1] for i in range(1,len(quakeTimes))]\n",
    "    #return np.array(retList)\n",
    "    return retList\n",
    "\n",
    "interQuakesSecs = interQuakeTimes(sorted([x[4] for x in myProcessedList]))\n",
    "len(interQuakesSecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[531.0, 551.0, 764.0, 294.0, 678.0, 1538.0, 376.0, 364.0, 208.0, 242.0]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interQuakesSecs[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "There is a lot of data here, so let's use an interactive plot to do the non-parametric DF estimation just for some of the last data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46262d9866544d8189d1d2aceabc137e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Interactive function <function _ at 0x7f3f041dd268> with 2 widgets\n",
       "  takeLast: SelectionSlider(description='ta…"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@interact\n",
    "def _(takeLast=(500,(0..min(len(interQuakesSecs),1999))), alpha=input_box(0.05.n(digits=3),label='$\\\\alpha$')):\n",
    "    '''Interactive function to plot the edf estimate and confidence bands for inter earthquake times.'''\n",
    "    if takeLast > 0 and alpha > 0 and alpha < 1:\n",
    "        lastInterQuakesSecs = interQuakesSecs[len(interQuakesSecs)-takeLast:len(interQuakesSecs)]\n",
    "        interQuakePoints = makeEDFPoints(lastInterQuakesSecs)\n",
    "        p=ecdfPointsPlot(interQuakePoints, lines_only=true)\n",
    "        epQuakes = calcEpsilon(alpha, len(lastInterQuakesSecs))\n",
    "        interQuakePointsLower = makeEDFPoints(lastInterQuakesSecs, offset=-epQuakes)\n",
    "        lowerQuakesBound = ecdfPointsPlot(interQuakePointsLower, colour='green', lines_only=true)\n",
    "        interQuakePointsUpper = makeEDFPoints(lastInterQuakesSecs, offset=epQuakes)\n",
    "        upperQuakesBound = ecdfPointsPlot(interQuakePointsUpper, colour='green', lines_only=true)\n",
    "        show(p + lowerQuakesBound + upperQuakesBound, figsize=[6,3])\n",
    "    else:\n",
    "        print (\"check your input values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Plug-in Estimator: A nonparametric Point Estimator \n",
    "\n",
    "A function $h$ of another function $g$ is called a *functional*. A **statistical functional** $T(F)$ is any function of the distribution function $F$.\n",
    "\n",
    "Suppose we are not directly interested in estimating the unknown $F^*$ in the experiment with $n$ IID samples: \n",
    "$$X_1,\\ldots,X_n \\overset{IID}{\\sim}F^* \\in \\{ \\text{ all DFs } \\}$$\n",
    "But rather in estimating a statistical functional of interest, say, $\\theta^* := T(F^*)$.\n",
    "\n",
    "NOTE: Here $\\theta^*$ is not a parameter, but just some unknown quantity of interest that can be obtained as a statistical functional $T(F^*)$ of the unknown $F^*$.\n",
    "\n",
    "Some examples of statistical functionals you have already seen include:\n",
    "\n",
    "-  The expectation of $g(X)$ is a functional as it merely involves the integration against $dF(x)$\n",
    "  - Mean with $g(X)=X$: \n",
    "    $$E(X) = \\int x \\, d F(x) = \\begin{cases} \\int x \\, f(x) dx & \\text{ if $X$ is a continuous distribution with probability density function $f(x)$}\\\\ \\sum_x x \\, f(x) & \\text{ if $X$ is a discrete distribution with probability mass function $f(x)$}\\end{cases}$$\n",
    "  - Variance with $g(X)=(X-E(X))^2$:\n",
    "    $$V(X) = \\int (x-E(X))^2 \\, d F(x)$$\n",
    "- Another interesting statistical function is the median:\n",
    "  - Median as the inverse of the DF at $0.5$:\n",
    "    $$\\mathrm{median}(X) = F^{[-1]}(0.5)$$\n",
    "\n",
    "By substituting in the point estimate $\\widehat{F}_n$ for the unknown $F^*$, we can obtain the **plug-in estimator** as:\n",
    "$$\n",
    "\\boxed{\n",
    "\\widehat{\\Theta}_n = T(\\widehat{F}_n) \\quad\n",
    "\\text{ of the quantity of interest } \\, \\theta^* := T(F^*)\n",
    "}\n",
    "$$\n",
    "In other words, just plug-in $\\widehat{F}_n$ for the unknown $F^*$ in $T(F^*) =: \\theta^*$. \n",
    "\n",
    "If $T(F)=\\int r(x) \\, d F(x)$ for some function $r(x)$ then $T$ is called a **linear functional** since $T$ is linear in its arguments: $T(aF+bG)=aT(F)+bT(G)$, where $(a,b) \\in \\mathbb{R}^2$.\n",
    "\n",
    "Thus, the plug-in estimator for any linear statistical functional $\\theta^* = T(F^*)=\\int r(x) dF^*(x)$ is:\n",
    "\n",
    "$$\n",
    "\\boxed{\\widehat{\\Theta}_n = T(\\widehat{F}_n) = \\frac{1}{n} \\sum_{i=1}^n r(x_i)}\n",
    "$$\n",
    "because:\n",
    "$$\n",
    "\\widehat{\\Theta}_n = T(\\widehat{F}_n) = \\int r(x) d \\widehat{F}_n(x) = \\sum_{i=1}^n r(x_i) \\, P(X=x_i) = \\sum_{i=1}^n r(x_i) \\, f(x_i) = \\sum_{i=1}^n r(x_i) \\, \\frac{1}{n} = \\frac{1}{n} \\sum_{i=1}^n r(x_i)\n",
    "$$\n",
    "\n",
    "Sample mean and sample variance are actually examples of plug-in estimators of the linear statistical functionals $E(X)$ and $V(X)$\n",
    "  - Sample Mean: \n",
    "    $$\\overline{X}_n = \\int x \\, d \\widehat{F}_n(x) = \\frac{1}{n} \\sum_{i=1}^n x_i$$\n",
    "  - Sample Variance:\n",
    "    $$\\mathrm{var}(X_n) = \\int (x-\\overline{X}_n)^2 \\, d \\widehat{F}_n(x) = \\frac{1}{n} \\sum_{i=1}^n \\left( x_i - \\overline{X}_n\\right)^2$$\n",
    "    \n",
    "Quantiles generalise the notion of a median and are very useful statistics when we want to summarise the values taken by a random variable. For a given $q \\in (0,1)$, **the $q$-th quantile** of a random variable with DF $F$ is the statistical functional: \n",
    "$$T(F)={F}^{[-1]}(q) := \\inf\\{x: F(x) \\geq q\\}$$\n",
    "We can use the plug-in estimator:\n",
    "$$\\widehat{F}_n^{[-1]}(q) := \\inf\\{x: \\widehat{F}_n(x) \\geq q\\}$$\n",
    "to estimate $T(F)={F}^{[-1]}(q)$, and $\\widehat{F}_n^{[-1]}(q)$ is called the **$q$-th sample quantile**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Example:  Estimating quantiles of inter-earthquake times\n",
    "\n",
    "We can simply call the quantile function knowing that it is just a plug-in estimator of the $q$-th quantile.\n",
    "\n",
    "#### Quantiles and Percentiles\n",
    "If $q \\in [0,100]$ then we call $(\\frac{q}{100})$-th quantile as the $q$-th percentile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75.255"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "iQMinutes = np.array(interQuakesSecs)/60.0\n",
    "np.percentile(iQMinutes,95.0) # percentile\n",
    "# is in range (0,100) corresponding to quantile in (0,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "So only $5\\%=0.05$ of the inter-EQ times are longer than 75 minutes. If we assume this non-parametric model that  models the times between EQs as IID RVs, then the probability of not have any earthquakes in New Zealand (during the observation period) for longer than 150 minutes is no smaller than $0.05 \\times 0.05 = 0.0025 = 1/400$, a pretty unlikely event!\n",
    "\n",
    "Quantiles as plug-in estimates can give us a lot of basic insights into the data under the more general nonparametric view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum, maximum inter EQ-time in minues =  0.0 327.8\n",
      "(0.05-th, 0.50-th or median, 0.95-th) quantiles =  (1.3166666666666667, 15.516666666666667, 75.255)\n"
     ]
    }
   ],
   "source": [
    "print (\"minimum, maximum inter EQ-time in minues = \", min(iQMinutes), max(iQMinutes))\n",
    "\n",
    "print (\"(0.05-th, 0.50-th or median, 0.95-th) quantiles = \",\\\n",
    "(np.percentile(iQMinutes,5.0),np.percentile(iQMinutes,50.0), np.percentile(iQMinutes,95.0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Bootstrap for Confidence Sets in Nonparametric Experiments \n",
    "\n",
    "> This is an adaptation of *All of Statistics (2003)* by Wasserman of Carnegie Mellon University, Pittsburgh, PA, USA (one of the first universities that started presenting an integrated research and educational view of mathematical statistics and machine learning, along both theoretical and applied fronts).\n",
    "\n",
    "Plug-in estimators are point estimators. Next we will see a very general way to obtain set estimators of $\\theta^* = T(F^*)$, some statistical functional of interest, when we do not make any parametric assumptions about the underlying unknown distribution function $F^*$ in our IID experiment:\n",
    "\n",
    "$$X_1,\\ldots,X_n \\overset{IID}{\\sim}F^* \\in \\{ \\text{ all DFs } \\}$$\n",
    "\n",
    "NOTE: To get confidence sets (or intervals in 1D) we need to obtain estimated standard error $\\widehat{se}_n$ of our point estimator, such as the plug-in estimator, $\\widehat{\\Theta}_n$ of $\\theta^*=T(F^*)$. This is generally not possible. However, the bootstrap due to Efron gives us a helping hand.\n",
    "\n",
    "The **bootstrap** is a statistical method for estimating standard errors and confidence sets of statistics, such as estimators.\n",
    "\n",
    "Let $T_n := T_n((X_1,X_2,\\ldots,X_n))$ be a statistic, i.e, any function of the data $X_1,X_2,\\ldots,X_n \\overset{IID}{\\sim} F^*$.  \n",
    "\n",
    "Suppose we want to know its variance $V_{F^*}(T_n)$, which clearly depends on the fixed and possibly unknown DF $F^*$ (hence the subscripting by $F^*$ to emphasise this dependence).\n",
    "\n",
    "If our statistic $T_n$ is one with an analytically unknown variance, then we can use the bootstrap to estimate it.  \n",
    "\n",
    "The bootstrap idea has the following **two basic steps** with their underlying assumptions:\n",
    "\n",
    "- $\\mathsf{Step~1}$: *Estimate* $V_{F^*}(T_n)$ with $V_{\\widehat{F}_n}(T_n)$.\n",
    "  - Here we are assuming that $n$ is large enough so that $\\widehat{F}_n$ is a good approximation of $F^*$.(Glivenko-Cantelli Theorem and Dvoretzky-Kiefer-Wolfowitz Inequality)\n",
    "- $\\mathsf{Step~2}$: *Approximate* $V_{\\widehat{F}_n}(T_n)$ using simulated data from the \"Bootstrap World.\" \n",
    "  - Here we are assuming that the simulated data is available $B$ times for large enough $B$ to approximate $V_{\\widehat{F}_n}(T_n)$ (Law of Large Numbers)\n",
    "\n",
    "\n",
    "For example, if $T_n=\\overline{X}_n$, in $\\mathsf{Step~1}$, $V_{\\widehat{F}_n}(T_n) = s_n^2/n$, where $s_n^2=n^{-1} \\sum_{i=1}^n (x_i-\\overline{x}_n)$ is the sample variance and $\\overline{x}_n$ is the sample mean.  In this case,  $\\mathsf{Step~1}$ is enough.  However, when the statistic $T_n$ is more complicated (e.g. $T_n=\\widetilde{X}_n = F^{[-1]}(0.5)$), the sample median, then we may not be able to find a simple expression for $V_{\\widehat{F}_n}(T_n)$ and may need $\\mathsf{Step~2}$ of the bootstrap.\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\begin{aligned}\n",
    "\\text{Real World Data come from} &  F^* \\quad \\implies X_1,X_2,\\ldots,X_n & \\implies\n",
    "T_n((X_1,X_2,\\ldots,X_n))=t_n \\notag \\\\\n",
    "\\text{Bootstrap World Data come from} & \\widehat{F}_n \\quad  \\implies X^{\\bullet}_1,X^{\\bullet}_2,\\ldots,X^{\\bullet}_n & \\implies\n",
    "T_n((X^{\\bullet}_1,X^{\\bullet}_2,\\ldots,X^{\\bullet}_n))=t^{\\bullet}_n \\notag\n",
    "\\end{aligned}\n",
    "}\n",
    "$$\n",
    "\n",
    "Observe that drawing an observation from the empirical DF $\\widehat{F}_n$ is equivalent to drawing one point at random from the original data (think of the indices $[n] := \\{ 1,2,\\ldots,n \\}$ of the original data $X_1,X_2,\\ldots,X_n$ being drawn according to the equi-probable $de~Moivre(1/n,1/n,\\ldots,1/n)$ RV on $[n]$).  Thus, to simulate $X^{\\bullet}_1,X^{\\bullet}_2,\\ldots,X^{\\bullet}_n$ from $\\widehat{F}_n$, it is enough to drawn $n$ observations with replacement from the dataset $\\{X_1,X_2,\\ldots,X_n\\}$.\n",
    "\n",
    "In summary, the algorithm for Bootstrap Variance Estimation is:\n",
    "\n",
    "\n",
    "> - $\\mathsf{Step~1}$: Draw $X^{\\bullet}_1,X^{\\bullet}_2,\\ldots,X^{\\bullet}_n \\sim \\widehat{F}_n$\n",
    "> - $\\mathsf{Step~2}$: Compute $t_n^{\\bullet} = T_n((X^{\\bullet}_1,X^{\\bullet}_2,\\ldots,X^{\\bullet}_n))$\n",
    "> - $\\mathsf{Step~3}$: Repeat $\\mathsf{Step~1}$ and $\\mathsf{Step~2}$ $B$ times, for some large $B$, say $B>1000$, to get $t_{n,1}^{\\bullet}, t_{n,2}^{\\bullet},\\ldots,t_{n,B}^{\\bullet}$\n",
    "> - $\\mathsf{Step~4}$: Several ways of estimating the bootstrap confidence intervals are possible, we use the one based on percentiles:\n",
    ">  - The $1-\\alpha$ percentile-based bootstrap confidence interval is:\n",
    "$$\n",
    "C_n=[\\widehat{G^{\\bullet}}_{n}^{-1}(\\alpha/2),\\widehat{G^{\\bullet}}_{n}^{-1}(1-\\alpha/2)] ,\n",
    "$$\n",
    "> where $\\widehat{G^{\\bullet}}_{n}$ is the empirical DF of the bootstrapped $t_{n,1}^{\\bullet}, t_{n,2}^{\\bullet},\\ldots,t_{n,B}^{\\bullet}$ and $\\widehat{G^{\\bullet}}_{n}^{-1}(q)$ is the $q^{\\text{th}}$ sample quantile of $t_{n,1}^{\\bullet}, t_{n,2}^{\\bullet},\\ldots,t_{n,B}^{\\bullet}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Confidence Interval via Bootstrap - Median of inter-EQ Times in Minutes\n",
    "\n",
    "Use Bootstrap to obtain a 95% confidence interval of the median inter earth-quate time in minutes. Use this interval to test the null hypothesis that the median inter earth-quate time is 10 minutes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sample median =  15.516666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The inner 95% percentile based Confidence Interval for the Median = \n",
      "[ 15.183333333333334 , 15.817083333333333 ]\n"
     ]
    }
   ],
   "source": [
    "# Sample Exam Problem x - Solution\n",
    "# Be patient! this can take a couple minutes; reduce B to 100 to save time\n",
    "import numpy as np\n",
    "iQMinutes = np.array(interQuakesSecs)/60.0\n",
    "sampleMedian = np.percentile(iQMinutes,50.0) # median \n",
    "print (\"The sample median = \",sampleMedian)\n",
    "\n",
    "B = 1000 # Number of Bootstrap replications\n",
    "n = len(iQMinutes) # sample size of the original dataset\n",
    "bootstrappedSampleMedians=[] # list to store the sample medians from each bootstrapped data\n",
    "for b in range(B):\n",
    "    #sample indices at random between 0 and len(iQMinutes)-1 to make the bootstrapped dataset\n",
    "    randIndices=[randint(0,n-1) for i in range(n)] \n",
    "    bootstrappedDataset = iQMinutes[randIndices] # resample with replacement from original dataset\n",
    "    bootstrappedMedian = np.percentile(bootstrappedDataset,50.0)\n",
    "    bootstrappedSampleMedians.append(bootstrappedMedian)\n",
    "\n",
    "lower95BootstrapCIForMedian = np.percentile(bootstrappedSampleMedians,2.5)\n",
    "upper95BootstrapCIForMedian = np.percentile(bootstrappedSampleMedians,97.5)\n",
    "\n",
    "print (\"The inner 95% percentile based Confidence Interval for the Median = \")\n",
    "print (\"[ \"+str(lower95BootstrapCIForMedian) + \" , \" + str(upper95BootstrapCIForMedian) +\" ]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Using numpy to bootstrap, a faster method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sample median =  15.516666666666667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The inner 95% percentile based Confidence Interval for the Median = \n",
      "[ 15.133333333333333 , 15.866666666666667 ]\n"
     ]
    }
   ],
   "source": [
    "# Sample Exam Problem x - Solution\n",
    "# Be patient! this can take a couple minutes; reduce B to 100 to save time\n",
    "import numpy as np\n",
    "iQMinutes = np.array(interQuakesSecs)/60.0\n",
    "sampleMedian = np.percentile(iQMinutes,50.0) # median \n",
    "print (\"The sample median = \",sampleMedian)\n",
    "\n",
    "B = 1000 # Number of Bootstrap replications\n",
    "n = len(iQMinutes) # sample size of the original dataset\n",
    "bootstrappedSampleMedians=[] # list to store the sample medians from each bootstrapped data\n",
    "for b in range(B):\n",
    "    #sample indices at random between 0 and len(iQMinutes)-1 to make the bootstrapped dataset\n",
    "    bootstrappedDataset = np.random.choice(iQMinutes,size=n,replace=True) # resample with replacement from original dataset\n",
    "    bootstrappedMedian = np.percentile(bootstrappedDataset,50.0)\n",
    "    bootstrappedSampleMedians.append(bootstrappedMedian)\n",
    "\n",
    "lower95BootstrapCIForMedian = np.percentile(bootstrappedSampleMedians,2.5)\n",
    "upper95BootstrapCIForMedian = np.percentile(bootstrappedSampleMedians,97.5)\n",
    "\n",
    "print (\"The inner 95% percentile based Confidence Interval for the Median = \")\n",
    "print (\"[ \"+str(lower95BootstrapCIForMedian) + \" , \" + str(upper95BootstrapCIForMedian) +\" ]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Testing via Bootstrap \n",
    "\n",
    "We can use bootstrap-based confidence interval in conjunction with Wald test in nonparametric experiments just as we used confidence interval from an asymptotically normal estimator such as the MLE in the parametric experiments.\n",
    "\n",
    "#### The $\\mathsf{size}$ Wald test from bootstrapped $1-\\alpha$ Confidence Interval $C_n$ for a statistic $t$\n",
    "\n",
    "The $\\mathsf{size}$ $\\alpha$ Wald test rejects:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\text{ $H_0: t^*=t_0$ versus $H_1: t^* \\neq t_0$ if and only if $t_0 \\notin C_n := C_n=[\\widehat{G^{\\bullet}}_{n}^{-1}(\\alpha/2),\\widehat{G^{\\bullet}}_{n}^{-1}(1-\\alpha/2)]$.\n",
    "}}\n",
    "$$\n",
    "\n",
    "$$\\boxed{\\text{Therefore, testing the hypothesis is equivalent to verifying whether the null value $t_0$ is in the bootstrapped confidence interval.}}$$\n",
    "\n",
    "\n",
    "\n",
    "#### Example: Wald Test that median of inter-EQ times is 10 minutes\n",
    "\n",
    "We reject the null hypothesis that the median is $10$ minutes, if $10$ is not inside the 95% confidence interval for the median we just obtained via bootstrap. This is just like the Wald test except that we use the bootstrap instead of the asymptotic normality of the maximum likelihood estimator.\n",
    "\n",
    "Let us see this more explicitly in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The null value for median = 10.0000000000000\n",
      "is NOT inside the 95% Bootstrapped Confidence Interval: [ 15.133333333333333 , 15.866666666666667 ]\n",
      "        Therefore, we reject the null hypothesis H_0: median = 10.0000000000000\n"
     ]
    }
   ],
   "source": [
    "### Example: Wald Test that median of inter-EQ times is 10 minutes\n",
    "#### Be patient if you see a 'In [*]' to the left top corner of the cell ... your computer is working...\n",
    "nullValueForMedian = 10.0\n",
    "\n",
    "# check if the null value is in the 95% CI or not to fail to reject or reject the null hypothesis\n",
    "if nullValueForMedian >= lower95BootstrapCIForMedian and nullValueForMedian <= upper95BootstrapCIForMedian:\n",
    "    print (\"The null value for median = \"+str(nullValueForMedian)+\\\n",
    "    \"\\nis inside the 95% Bootstrapped Confidence Interval: \"+\\\n",
    "    \"[ \"+str(lower95BootstrapCIForMedian) + \" , \" + str(upper95BootstrapCIForMedian) +\" ]\")\n",
    "    print (\"        Therefore, we fail to reject the null hypothesis H_0: median=1.0\")\n",
    "else:\n",
    "    print (\"The null value for median = \"+str(nullValueForMedian)+\\\n",
    "    \"\\nis NOT inside the 95% Bootstrapped Confidence Interval: \"+\\\n",
    "    \"[ \"+str(lower95BootstrapCIForMedian) + \" , \" + str(upper95BootstrapCIForMedian) +\" ]\")\n",
    "    print (\"        Therefore, we reject the null hypothesis H_0: median = \"+str(nullValueForMedian))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Making a Generic Bootstrap Function for $1-\\alpha$ Confidence Interval of any Statistic\n",
    "\n",
    "Making a generic function `makeBootstrappedConfidenceIntervalOfStatisticT` as done below can be helpful in other problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def makeBootstrappedConfidenceIntervalOfStatisticT(dataset, statT, alpha, B=100):\n",
    "    '''make a bootstrapped 1-alpha confidence interval for ANY given statistic statT \n",
    "    from the dataset with B Bootstrap replications for 0 < alpha < 1, and \n",
    "    return lower CI, upper CI, bootstrapped_samples '''\n",
    "    n = len(dataset) # sample size of the original dataset\n",
    "    bootstrappedStatisticTs=[] # list to store the statistic T from each bootstrapped data\n",
    "    for b in range(B):\n",
    "        #sample indices at random between 0 and len(iQMinutes)-1 to make the bootstrapped dataset\n",
    "        #randIndices=[randint(0,n-1) for i in range(n)] \n",
    "        #bootstrappedDataset = dataset[randIndices] # resample with replacement from original dataset\n",
    "        bootstrappedDataset = np.random.choice(dataset,size=n,replace=True) # resample with replacement from original dataset\n",
    "        bootstrappedStatisticT = statT(bootstrappedDataset)\n",
    "        bootstrappedStatisticTs.append(bootstrappedStatisticT)\n",
    "    # noe get the [2.5%, 97.5%] percentile-based CI\n",
    "    alpaAsPercentage=alpha*100.0\n",
    "    lowerBootstrap1MinusAlphaCIForStatisticT = np.percentile(bootstrappedStatisticTs,alpaAsPercentage/2)\n",
    "    upperBootstrap1MinusAlphaCIForStatisticT = np.percentile(bootstrappedStatisticTs,100-alpaAsPercentage/2)\n",
    "    return (lowerBootstrap1MinusAlphaCIForStatisticT,upperBootstrap1MinusAlphaCIForStatisticT,\\\n",
    "            np.array(bootstrappedStatisticTs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Demonstrating the use of bootstrap for different statistics of interest\n",
    "\n",
    "We can obtain the bootstrap-based $1-\\alpha$ confidence intervals quite easily for any statistic and any dataset as demonstrated below.\n",
    "\n",
    "#### Bootstrapped $1-\\alpha$ Confidence Interval for Sample Median of inter-EQ Time in Minutes\n",
    "\n",
    "All we need to do is follow these **three steps to perform a bootstrap**:\n",
    "\n",
    "1. define a lambda expression for the statistic of interest: `statTMedian = lambda dataset : np.percentile(dataset,50.0)` \n",
    "- get the plugin estimate : `statTMedian = lambda dataset : np.percentile(dataset,50.0)`\n",
    "- and call the generic function we made:\n",
    ">   `lowerCIMedian,upperCIMedian,bootValuesMedian = \\`\n",
    ">    `                          makeBootstrappedConfidenceIntervalOfStatisticT(iQMinutes, statTMedian, alpha, B)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Plug-in Point Estimate of the Population Median of inter-EQ Times  =  15.516666666666667\n",
      "1-alpha Bootstrapped CI for the media of inter-EQ Times =  (15.257916666666667, 15.800833333333333)\n",
      "         where alpha =  0.050  and bootstrap replicates =  100\n"
     ]
    }
   ],
   "source": [
    "#### Be patient if you see a 'In [*]' to the left top corner of the cell ... your computer is working...\n",
    "# dataset\n",
    "iQMinutes = np.array(interQuakesSecs)/60.0\n",
    "\n",
    "# our statistic T as one example, say the 50-th percentile or median as  \n",
    "# via anonymous function / lambda expression\n",
    "statTMedian = lambda dataset : np.percentile(dataset,50.0)\n",
    "\n",
    "# now obtain the bootstrapped 1-alpha confidence interval with alpha=0.5, i.e. 95% conf Interval\n",
    "alpha=0.05\n",
    "B=100 # number of bootstrap samples\n",
    "\n",
    "# plug-in point estimate of population median of inter-EQ times\n",
    "plugInEstimateOfMedian = statTMedian(iQMinutes)\n",
    "\n",
    "# Let's call our convenient function\n",
    "# and get the bootstrapped samples and build 1-alpha confidence interval\n",
    "lowerCIMedian,upperCIMedian,bootValuesMedian = \\\n",
    "           makeBootstrappedConfidenceIntervalOfStatisticT(iQMinutes, statTMedian, alpha, B)\n",
    "\n",
    "# print the results\n",
    "print (\"The Plug-in Point Estimate of the Population Median of inter-EQ Times  = \", plugInEstimateOfMedian)\n",
    "print (\"1-alpha Bootstrapped CI for the media of inter-EQ Times = \", (lowerCIMedian,upperCIMedian))\n",
    "print (\"         where alpha = \",alpha.n(digits=2),\" and bootstrap replicates = \", B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "5ecb4d40c399a03a04b2206719b408ee8c9f3084",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pylab\n",
    "pylab.clf() # clear current figure\n",
    "Bins=8 # Number of histogram bins\n",
    "n, bins, patches = pylab.hist(bootValuesMedian, Bins, density=true) \n",
    "pylab.ylabel('normalised count')\n",
    "pylab.title('Normalised histogram of bootstrapped samples of the plug-in estimate of median inter-EQ time')\n",
    "pylab.axvline(x=plugInEstimateOfMedian, linewidth=2, color='k') # plot vertical line at plug-in point estimate\n",
    "pylab.axvline(x=lowerCIMedian, linewidth=2, color='r') # plot vertical line at lower CI at (alpha/2)-th Quantile\n",
    "pylab.axvline(x=upperCIMedian, linewidth=2, color='r') # plot vertical line at upper CI at (1-alpha/2)-th Quantile\n",
    "#pylab.savefig('myHist') # to actually save the figure for subsequent extraction elsewhere\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### CAUTION: Bootstrap is justified if two of its assumptions are satisfied:\n",
    "\n",
    "Recall assumptions in **the two basic steps** in the Bootstrap from above (notebook `11.ipynb`): \n",
    "1. $\\mathsf{Step~1}$: $n$ is large enough, so that the empirical DF $\\widehat{F}_n$ is a good approximation for the unknown $F^*$ in the model $X_1,X_2,\\ldots,X_n \\overset{IID}{\\sim} F^*$, and\n",
    "- $\\mathsf{Step~2}$: $B$, the number of bootstrap samples is large enough.\n",
    "\n",
    "So it is a good idea to increase $B$ to $1000$ or perhaps more (depending on the problem and available computational resources) to see how the results are affected. Remember, the confidence intervals will slightly change due to random seed in the simulation and your values for $n$ and $B$.\n",
    "\n",
    "NOTE: When the dataset is much larger in size (order of billions or hundreds of billions) or not IID real-valued but perhaps dependent (eg. time series models) then there are other sub-sampling schemes that one may need to use under further assumptions on the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Sample Exam Problem 7\n",
    "\n",
    "Obtain the plug-in estimate of the population median of inter-EQ times in minutes stored in the array `iQMinutes`. \n",
    "Using $1000$ bootstrap replicates $B$, obtain the the 95% confidence interval for the median inter-EQ time in minutes.\n",
    "Using a Wald-like Test based on the bootstrapped 95% confidence interval, test the null hypothesis that the inter-EQ time is 20 minutes (just report the finding of your test through the boolean `RejectedH0_sampleMedianIs20min` ).\n",
    "\n",
    "NOTE: Make Sure you run the `REQUIRED-CELL` before trying the Problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sample Exam Problem 7 - REQUIRED-CELL\n",
    "\n",
    "# DO NOT MODIFY this cell \n",
    "# Evaluate this cell before trying this PROBLEM so that the required functions and variables are loaded\n",
    "\n",
    "import numpy as np\n",
    "## Be Patient! - This will take more time, about a minute or so\n",
    "###############################################################################################\n",
    "def getLonLatMagDepTimes(NZEQCsvFileName):\n",
    "    '''returns longitude, latitude, magnitude, depth and the origin time as unix time\n",
    "    for each observed earthquake in the csv filr named NZEQCsvFileName'''\n",
    "    from datetime import datetime\n",
    "    import time\n",
    "    from dateutil.parser import parse\n",
    "    import numpy as np\n",
    "    \n",
    "    with open(NZEQCsvFileName) as f:\n",
    "        reader = f.read() \n",
    "        dataList = reader.split('\\n')\n",
    "        \n",
    "    myDataAccumulatorList =[]\n",
    "    for data in dataList[1:-1]:\n",
    "        dataRow = data.split(',')\n",
    "        myTimeString = dataRow[2] # origintime\n",
    "        # let's also grab longitude, latitude, magnitude, depth\n",
    "        myDataString = [dataRow[4],dataRow[5],dataRow[6],dataRow[7]]\n",
    "        try: \n",
    "            myTypedTime = time.mktime(parse(myTimeString).timetuple())\n",
    "            myFloatData = [float(x) for x in myDataString]\n",
    "            myFloatData.append(myTypedTime) # append the processed timestamp\n",
    "            myDataAccumulatorList.append(myFloatData)\n",
    "        except TypeError as e: # error handling for type incompatibilities\n",
    "            print ('Error:  Error is ', e)\n",
    "    #return np.array(myDataAccumulatorList)\n",
    "    return myDataAccumulatorList\n",
    "\n",
    "myProcessedList = getLonLatMagDepTimes('data/earthquakes.csv')\n",
    "\n",
    "def interQuakeTimes(quakeTimes):\n",
    "    '''Return a list inter-earthquake times in seconds from earthquake origin times\n",
    "    Date and time elements are expected to be in the 5th column of the array\n",
    "    Return a list of inter-quake times in seconds. NEEDS sorted quakeTimes Data'''\n",
    "    import numpy as np\n",
    "    retList = []\n",
    "    if len(quakeTimes) > 1:\n",
    "        retList = [quakeTimes[i]-quakeTimes[i-1] for i in range(1,len(quakeTimes))]\n",
    "    #return np.array(retList)\n",
    "    return retList\n",
    "\n",
    "def makeBootstrappedConfidenceIntervalOfStatisticT(dataset, statT, alpha, B=100):\n",
    "    '''make a bootstrapped 1-alpha confidence interval for ANY given statistic statT \n",
    "    from the dataset with B Bootstrap replications for 0 < alpha < 1, and \n",
    "    return lower CI, upper CI, bootstrapped_samples '''\n",
    "    n = len(dataset) # sample size of the original dataset\n",
    "    bootstrappedStatisticTs=[] # list to store the statistic T from each bootstrapped data\n",
    "    for b in range(B):\n",
    "        #sample indices at random between 0 and len(iQMinutes)-1 to make the bootstrapped dataset\n",
    "        randIndices=[randint(0,n-1) for i in range(n)] \n",
    "        bootstrappedDataset = dataset[randIndices] # resample with replacement from original dataset\n",
    "        bootstrappedStatisticT = statT(bootstrappedDataset)\n",
    "        bootstrappedStatisticTs.append(bootstrappedStatisticT)\n",
    "    # noe get the [2.5%, 97.5%] percentile-based CI\n",
    "    alpaAsPercentage=alpha*100.0\n",
    "    lowerBootstrap1MinusAlphaCIForStatisticT = np.percentile(bootstrappedStatisticTs,alpaAsPercentage/2)\n",
    "    upperBootstrap1MinusAlphaCIForStatisticT = np.percentile(bootstrappedStatisticTs,100-alpaAsPercentage/2)\n",
    "    return (lowerBootstrap1MinusAlphaCIForStatisticT,upperBootstrap1MinusAlphaCIForStatisticT,\\\n",
    "            np.array(bootstrappedStatisticTs))\n",
    "\n",
    "interQuakesSecs = interQuakeTimes(sorted([x[4] for x in myProcessedList]))\n",
    "iQMinutes = np.array(interQuakesSecs)/60.0\n",
    "###############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sample Exam Problem 7\n",
    "# first evaluate the REQUIRED-CELL above\n",
    "# Do NOT change the variable or function Names - Just replace XXXs\n",
    "# now obtain the bootstrapped 1-alpha confidence interval with alpha=0.5, i.e. 95% conf Interval\n",
    "alpha=0.05\n",
    "B=1000 # number of bootstrap samples=1000\n",
    "# our statistic T as one example, say the 50-th percentile or median as  \n",
    "# via anonymous function / lambda expression\n",
    "statTMedian = lambda dataset : XXXX\n",
    "\n",
    "# plug-in point estimate of population median of inter-EQ times\n",
    "plugInEstimateOfMedian = statTMedian(iQMinutes)\n",
    "\n",
    "# Let's call our convenient function\n",
    "# and get the bootstrapped samples and build 1-alpha confidence interval\n",
    "lowerCIMedian,upperCIMedian,bootValuesMedian = \\\n",
    "           makeBootstrappedConfidenceIntervalOfStatisticT(XXX, XXX, alpha, B)\n",
    "\n",
    "# print in more details if you want to see if your values make sense\n",
    "print (\"The Plug-in Point Estimate of the Population Median of inter-EQ Times  = \", plugInEstimateOfMedian)\n",
    "print (\"1-alpha Bootstrapped CI for the media of inter-EQ Times = \", (lowerCIMedian,upperCIMedian))\n",
    "print (\"         where alpha = \",alpha.n(digits=2),\" and bootstrap replicates = \", B)\n",
    "\n",
    "print (plugInEstimateOfMedian,lowerCIMedian,upperCIMedian)\n",
    "\n",
    "# Hypothesis test\n",
    "NullValueForMedian=20 # 20 minutes is the value of Median under the Null Hypothesis, H_0: Population Median = 20\n",
    "RejectedH0_sampleMedianIs20min = (NullValueForMedian <= XXX and NullValueForMedian >= XXX)\n",
    "\n",
    "if RejectedH0_sampleMedianIs20min:\n",
    "    print (\" RejectedH0_sampleMedianIs20min = True, so the Null Hypothesis is Rejected at size alpha = \",alpha)\n",
    "else:\n",
    "    print (\" RejectedH0_sampleMedianIs20min = False, so the Null Hypothesis is NOT Rejected at size alpha = \",alpha)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Sample Exam Problem 7 Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Plug-in Point Estimate of the Population Median of inter-EQ Times  =  15.516666666666667\n",
      "1-alpha Bootstrapped CI for the media of inter-EQ Times =  (15.224583333333333, 15.792916666666667)\n",
      "         where alpha =  0.050  and bootstrap replicates =  100\n",
      "15.516666666666667 15.224583333333333 15.792916666666667\n",
      " RejectedH0_sampleMedianIs20min = False, so the Null Hypothesis is NOT Rejected at size alpha =  0.0500000000000000\n"
     ]
    },
    {
     "data": {
      "image/png": "2a275525f866206e68b7eb3f1144909215446ee1",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample Exam Problem 7 - SOLUTION\n",
    "#### Be patient if you see a 'In [*]' to the left top corner of the cell ... your computer is working...\n",
    "# Do NOT change the variable or function Names - Just replace XXXs\n",
    "# now obtain the bootstrapped 1-alpha confidence interval with alpha=0.5, i.e. 95% conf Interval\n",
    "alpha=0.05\n",
    "B=100 # number of bootstrap samples=1000\n",
    "# our statistic T as one example, say the 50-th percentile or median as  \n",
    "# via anonymous function / lambda expression\n",
    "statTMedian = lambda dataset : np.percentile(dataset,50.0)\n",
    "\n",
    "# plug-in point estimate of population median of inter-EQ times\n",
    "plugInEstimateOfMedian = statTMedian(iQMinutes)\n",
    "\n",
    "# Let's call our convenient function\n",
    "# and get the bootstrapped samples and build 1-alpha confidence interval\n",
    "lowerCIMedian,upperCIMedian,bootValuesMedian = \\\n",
    "           makeBootstrappedConfidenceIntervalOfStatisticT(iQMinutes, statTMedian, alpha, B)\n",
    "\n",
    "# print in more details if you want to see if your values make sense\n",
    "print (\"The Plug-in Point Estimate of the Population Median of inter-EQ Times  = \", plugInEstimateOfMedian)\n",
    "print (\"1-alpha Bootstrapped CI for the media of inter-EQ Times = \", (lowerCIMedian,upperCIMedian))\n",
    "print (\"         where alpha = \",alpha.n(digits=2),\" and bootstrap replicates = \", B)\n",
    "\n",
    "print (plugInEstimateOfMedian,lowerCIMedian,upperCIMedian)\n",
    "\n",
    "# Hypothesis test\n",
    "NullValueForMedian=20 # 20 minutes is the value of Median under the Null Hypothesis, H_0: Population Median = 20\n",
    "\n",
    "RejectedH0_sampleMedianIs20min = (NullValueForMedian <= upperCIMedian and NullValueForMedian >= lowerCIMedian)\n",
    "\n",
    "if RejectedH0_sampleMedianIs20min:\n",
    "    print (\" RejectedH0_sampleMedianIs20min = True, so the Null Hypothesis is Rejected at size alpha = \",alpha)\n",
    "else:\n",
    "    print (\" RejectedH0_sampleMedianIs20min = False, so the Null Hypothesis is NOT Rejected at size alpha = \",alpha)\n",
    "\n",
    "# This is just extra stuff you don't need to do for this sample exam problem!\n",
    "import pylab\n",
    "pylab.clf() # clear current figure\n",
    "Bins=10 # Number of histogram bins\n",
    "n, bins, patches = pylab.hist(bootValuesMedian, Bins, density=true) \n",
    "pylab.ylabel('normalised count')\n",
    "pylab.title('Normalised histogram of bootstrapped samples of the plug-in estimate of median inter-EQ time')\n",
    "pylab.axvline(x=plugInEstimateOfMedian, linewidth=2, color='k') # plot vertical line at plug-in point estimate\n",
    "pylab.axvline(x=lowerCIMedian, linewidth=2, color='r') # plot vertical line at lower CI at (alpha/2)-th Quantile\n",
    "pylab.axvline(x=upperCIMedian, linewidth=2, color='r') # plot vertical line at upper CI at (1-alpha/2)-th Quantile\n",
    "#pylab.savefig('myHist') # to actually save the figure for subsequent extraction elsewhere\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Correlation: A Bivariate Nonparametric Bootstrap\n",
    "\n",
    "Here is a classical data set used by Bradley Efron at Stanford University's Statistics Department (the inventor of bootstrap) to illustrate the method.\n",
    "\n",
    "The data are LSAT (Law School Admission Test in the USA) scores and GPA (grade point average) of just fifteen individuals. The inference task involved assisting the Admissions Office with their evidence-based investigations on their admissions policies.\n",
    "\n",
    "Thus, we have bivariate data of the form $(Y_i,Z_i)$, where $Y_i={\\rm LSAT}_i$ and $Z_i={\\rm GPA}_i$.  For example, the first individual had an LSAT score of  $y_1=576$ and a GPA of $z_1=3.39$ while the fifteenth individual had an LSAT score of $y_{15}=594$ and a GPA of $z_{15}=3.96$.  \n",
    "\n",
    "We supose that the bivariate data has the following IID bivariate model:\n",
    "\n",
    "$$\\boxed{(Y_1,Z_1),(Y_2,Z_2),\\ldots,(Y_{15},Z_{15})  \\overset{IID}{\\sim} F^* \\in \\{ \\text{all bivariate DFs} \\}}$$\n",
    "\n",
    "This is a *bivariate nonparametric experiment* and its bivariate data is plotted below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "31cc7a384311e4274006c68bbff5df21d2e4f5f9",
      "text/plain": [
       "Graphics object consisting of 1 graphics primitive"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "LSAT=np.array([576, 635, 558, 578, 666, 580, 555, 661, 651, 605, 653, 575, 545, 572, 594]) # LSAT data\n",
    "GPA=np.array([3.39, 3.30, 2.81, 3.03, 3.44, 3.07, 3.00, 3.43, 3.36, 3.13, 3.12, 2.74, 2.76, 2.88, 3.96]) # GPA data\n",
    "# use np.vstack to vertically stack a sequence of arrays into a matrix of stacked rows\n",
    "# see https://docs.scipy.org/doc/numpy/reference/generated/numpy.vstack.html#numpy.vstack\n",
    "LSATGPA_data = np.vstack((LSAT,GPA)) # this is a 2D array of matric now\n",
    "p = points(zip(LSATGPA_data[0,:],LSATGPA_data[1,:]))\n",
    "p.show(figsize=(8,3),axes_labels=['$Y_i$ (LSAT scores)','$Z_i$ (GPA)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Plug-in Point Estimate of the Correlation\n",
    "\n",
    "The law school was interested in the **correlation** between the GPA and LSAT scores:\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\theta^* = \\frac{\\int \\int (y-E(Y))(z-E(Z))dF(y,z)}{\\sqrt{\\int (y-E(Y))^2 dF(y) \\int (z-E(Z))^2 dF(z)}}\n",
    "}\n",
    "$$\n",
    "\n",
    "The *plug-in estimate of the population correlation* $\\theta^*$ is the *sample correlation*:\n",
    "$$\n",
    "\\boxed{\n",
    "\\widehat{\\Theta}_n = \\frac{\\sum_{i=1}^n(Y_i-\\overline{Y}_n)(Z_i-\\overline{Z}_n)}{\\sqrt{\\sum_{i=1}^n(Y_i-\\overline{Y}_n)^2 \\sum_{i=1}^n(Z_i-\\overline{Z}_n)^2}}\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Sample Correlation Matrix is\n",
      "[[1.         0.54591892]\n",
      " [0.54591892 1.        ]]\n",
      "For a pair of variables we can extract the right entry of the symmetric Sample Correlation Matrix:\n",
      "0.5459189161795887\n"
     ]
    }
   ],
   "source": [
    "# the sample correlation matrix, it's symmetric with diagonal elements as 1\n",
    "SampleCorrelationMatrix = np.corrcoef(LSATGPA_data) \n",
    "print (\"The Sample Correlation Matrix is\")\n",
    "print (SampleCorrelationMatrix)\n",
    "print (\"For a pair of variables we can extract the right entry of the symmetric Sample Correlation Matrix:\")\n",
    "SampleCorrelation_GPA_LSAT = SampleCorrelationMatrix[(0,1)]\n",
    "print (SampleCorrelation_GPA_LSAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### The Bootstrapped $1-\\alpha$ Confidence Interval of the Sample Correlation and a Test\n",
    "\n",
    "In order to obtain the $1-\\alpha$ confidence interval of the plug-in estimate of the population correlation, we can quickly tap into our function thousand bootstrapped data sets.\n",
    "\n",
    "Then we can reject (or fail to reject) the null hypothesis that the true correlation coefficient is $0$ if $0$ is not contained in the bootstrapped 95% confidence interval (akin to a Wald Test, but with the nonparametric model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 is not inside the 1-alpha bootstrapped confidence interval for the correlation coefficient:\n",
      "   So, we reject the Null Hypothesis that the population correlation coefficient = 0\n"
     ]
    }
   ],
   "source": [
    "# use the right lamba expression for the statistic of interest\n",
    "statTSampleCorrCoeff = lambda dataMatrix : np.corrcoef(dataMatrix)[(0,1)] #statistic of interest\n",
    "alpha=0.05\n",
    "B=1000 # number of bootstrap samples\n",
    "# get bootstrapped 1-alpha confidence Interval for statT99thPercentile\n",
    "lowerCITcor,upperCITcor,bootValuesTcor = \\\n",
    "                 makeBootstrappedConfidenceIntervalOfStatisticT(LSATGPA_data, statTSampleCorrCoeff, alpha, B) \n",
    "\n",
    "if 0>=lowerCITcor and 0<=upperCITcor:\n",
    "    print (\"0 is inside the 1-alpha bootstrapped confidence interval for the correlation coefficient:\")\n",
    "    print (\"   So, we fail to reject the Null Hypothesis that the population correlation coefficient = 0\")\n",
    "else:\n",
    "    print (\"0 is not inside the 1-alpha bootstrapped confidence interval for the correlation coefficient:\")\n",
    "    print (\"   So, we reject the Null Hypothesis that the population correlation coefficient = 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Why is testing whether correlation = 0 is of interest?\n",
    "#### Correlation Versus Causation - Proceed with Extreme Caution!\n",
    "\n",
    "The following image from [https://en.wikipedia.org/wiki/Correlation_and_dependence](https://en.wikipedia.org/wiki/Correlation_and_dependence) shows:\n",
    "> Several sets of (x, y) points, with the (Pearson's) correlation coefficient of x and y for each set. Note that the correlation reflects the noisiness and direction of a linear relationship (top row), but not the slope of that relationship (middle), nor many aspects of nonlinear relationships (bottom). N.B.: the figure in the center has a slope of 0 but in that case the correlation coefficient is undefined because the variance of Y is zero.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Correlation_examples2.svg/1200px-Correlation_examples2.svg.png\" alt=\"Correlation examples2.svg\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Nonparametric Hypothesis Testing \n",
    "### Are two samples from the same distribution or not?\n",
    "\n",
    "What if we are not interested in estimating $F^*$ itself, but we are interested in scientificially investigating whether two distributions are the same.  For example, perhaps, whether the distribution of earthquake magnitudes was the same in April as it was in March or whether human subjects under a clinical trial are reacting differently two a new drug to treat some disease (typically you have a control group that does not get the treatment and the treated group).\n",
    "\n",
    "You already have the means to do by using a Wald test of say the difference in sample means of the two samples, or correlation coefficient, etc for parametric or nonparametric experiments.\n",
    "\n",
    "We will next see a useful nonparametric approach to this problem.\n",
    "\n",
    "\n",
    "### Permutation Testing\n",
    "\n",
    "A Permuation Test is a **non-parametric exact** method for testing whether two distributions are the same based on samples from each of them. In industry analogs and variants of permutation testing is known as *A/B Testing* in industry today.\n",
    "\n",
    "What do we mean by \"non-parametric exact\"?  It is non-parametric because we do not impose any parametric assumptions.  It is exact because it works for any sample size.\n",
    "\n",
    "Formally, we suppose that: \n",
    "$$ X_1,X_2,\\ldots,X_m \\overset{IID}{\\sim} F^* \\quad \\text{and} \\quad X_{m+1}, X_{m+2},\\ldots,X_{m+n} \\overset{IID}{\\sim} G^* \\enspace , $$\n",
    "are two sets of independent samples where the possibly unknown DFs \n",
    "$F^*,\\,G^* \\in \\{ \\text{all DFs} \\}$.\n",
    "\n",
    "(Notice that we have written it so that the subscripts on the $X$s run from 1 to $m+n$.)\n",
    "\n",
    "Now, consider the following hypothesis test: \n",
    "$$H_0: F^*=G^* \\quad \\text{versus} \\quad H_1: F^* \\neq G^* \\enspace . $$\n",
    "\n",
    "Our test statistic uses the observations in both both samples.  We want a test statistic that is a sensible one for the test, i.e., will be large when when $F^*$ is 'too different' from $G^*$\n",
    "\n",
    "So, let our test statistic $T(X_1,\\ldots,X_m,X_{m+1},\\ldots,X_{m+n})$ be say: \n",
    "$$\n",
    "T:=T(X_1,\\ldots,X_m,X_{m+1},\\ldots,X_{m+n})= \\text{abs} \\left( \\frac{1}{m} \\sum_{i=1}^m X_i - \\frac{1}{n} \\sum_{i=m+1}^n X_i \\right) \\enspace .\n",
    "$$\n",
    "\n",
    "(In words, we have chosen a test statistic that is the absolute value of the difference in the sample means.   Note the limitation of this:  if $F^*$ and $G^*$ have the same mean but different variances, our test statistic $T$ will not be large.)\n",
    "\n",
    "Then the idea of a permutation test is as follows:\n",
    "\n",
    "- Let $N:=m+n$ be the pooled sample size and consider all $N!$ permutations of the observed data $x_{obs}:=(x_1,x_2,\\ldots,x_m,x_{m+1},x_{m+2},\\ldots,x_{m+n})$.\n",
    "- For each permutation of the data compute the statistic $T(\\text{permuted data } x)$ and denote these $N!$ values of $T$ by $t_1,t_2,\\ldots,t_{N!}$.\n",
    "- Under $H_0: X_1,\\ldots,X_m,X_{m+1},\\ldots,X_{m+n} \\overset{IID}{\\sim}F^*=G^*$, each of the permutations of $x= (x_1,x_2,\\ldots,x_m,x_{m+1},x_{m+2},\\ldots,x_{m+n})$ has the same joint probability $\\prod_{i=1}^{m+n} f(x_i)$, where $f(x_i)$ is the density function corresponding to $F^*=G^*$, $f(x_i)=dF(x_i)=dG(x_i)$. \n",
    "- Therefore, the transformation of the data by our statistic $T$ also has the same probability over the values of $T$, namely $\\{t_1,t_2,\\ldots,t_{N!}\\}$. Let $\\mathbf{P}_0$ be this permutation distribution under the null hypothesis. $\\mathbf{P}_0$ is discrete and uniform over $\\{t_1,t_2,\\ldots,t_{N!}\\}$. \n",
    "- Let $t_{obs} := T(x_{obs})$ be the observed value of the test statistic.\n",
    "- Assuming we reject $H_0$ when $T$ is large, the P-value = $\\mathbf{P}_0 \\left( T \\geq t_{obs} \\right)$\n",
    "- Saying that $\\mathbf{P}_0$ is discrete and uniform over $\\{t_1, t_2, \\ldots, t_{N!}\\}$ says that each possible permutation has an equal probabability of occuring (under the null hypothesis).  There are $N!$ possible permutations and so the probability of any individual permutation is $\\frac{1}{N!}$\n",
    "\n",
    "$$\n",
    "\\text{P-value} = \\mathbf{P}_0 \\left( T \\geq t_{obs} \\right) = \\frac{1}{N!} \\left( \\sum_{j=1}^{N!} \\mathbf{1} (t_j \\geq t_{obs}) \\right), \\qquad \\mathbf{1} (t_j \\geq t_{obs}) = \\begin{cases} 1 & \\text{if } \\quad t_j \\geq t_{obs} \\\\ 0 & \\text{otherwise} \\end{cases}\n",
    "$$\n",
    "\n",
    "This will make more sense if we look at some real data. \n",
    "\n",
    "### Permutation Testing with Shell Data\n",
    "\n",
    "In 2008, Guo Yaozong and Chen Shun collected data on the diameters of coarse venus shells from New Brighton beach for a course project.  They recorded the diameters for two samples of shells, one from each side of the New Brighton Pier.  The data is given in the following two cells.\n",
    "\n",
    "*NOTE - Real Data for Really Applied Statistics*: Under my guidance and assistance, students collected the shells on either side of the pier, cleaned the shells with warm soap water, rinsed and and treated with a diluted chlorine solution, used a taxonomy guide to identify the mollusc species and classified them manually. Then they focused on *Dosinia anus* or coarse venus shells. After the diluted chlorine treatment to disinfect microflora/fauna, the shells were washed, dried, placed on a graph paper, etched around the base with a mechanical pencil, and finally measured with a scale for the largest diameter within the outline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "leftSide = [52, 54, 60, 60, 54, 47, 57, 58, 61, 57, 50, 60, 60, 60, 62, 44, 55, 58, 55,\\\n",
    "            60, 59, 65, 59, 63, 51, 61, 62, 61, 60, 61, 65, 43, 59, 58, 67, 56, 64, 47,\\\n",
    "            64, 60, 55, 58, 41, 53, 61, 60, 49, 48, 47, 42, 50, 58, 48, 59, 55, 59, 50, \\\n",
    "            47, 47, 33, 51, 61, 61, 52, 62, 64, 64, 47, 58, 58, 61, 50, 55, 47, 39, 59,\\\n",
    "            64, 63, 63, 62, 64, 61, 50, 62, 61, 65, 62, 66, 60, 59, 58, 58, 60, 59, 61,\\\n",
    "            55, 55, 62, 51, 61, 49, 52, 59, 60, 66, 50, 59, 64, 64, 62, 60, 65, 44, 58, 63]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rightSide = [58, 54, 60, 55, 56, 44, 60, 52, 57, 58, 61, 66, 56, 59, 49, 48, 69, 66, 49,\\\n",
    "             72, 49, 50, 59, 59, 59, 66, 62, 44, 49, 40, 59, 55, 61, 51, 62, 52, 63, 39,\\\n",
    "             63, 52, 62, 49, 48, 65, 68, 45, 63, 58, 55, 56, 55, 57, 34, 64, 66, 54, 65,\\\n",
    "             61, 56, 57, 59, 58, 62, 58, 40, 43, 62, 59, 64, 64, 65, 65, 59, 64, 63, 65,\\\n",
    "             62, 61, 47, 59, 63, 44, 43, 59, 67, 64, 60, 62, 64, 65, 59, 55, 38, 57, 61,\\\n",
    "             52, 61, 61, 60, 34, 62, 64, 58, 39, 63, 47, 55, 54, 48, 60, 55, 60, 65, 41,\\\n",
    "             61, 59, 65, 50, 54, 60, 48, 51, 68, 52, 51, 61, 57, 49, 51, 62, 63, 59, 62,\\\n",
    "             54, 59, 46, 64, 49, 61]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(115, 139)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(leftSide), len(rightSide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "$(115 + 139)!$ is a very big number.   Lets start small, and take a subselection of the shell data to demonstrate the permutation test concept:  the first two shells from the left of the pier and the first one from the right:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[52, 54, 58]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rightSub = [52, 54]\n",
    "leftSub = [58]\n",
    "totalSample = rightSub + leftSub\n",
    "totalSample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "So now we are testing the hypotheses\n",
    "\n",
    "$$\\begin{array}{lcl}H_0&:& X_1,X_2,X_3 \\overset{IID}{\\sim} F^*=G^* \\\\H_1&:&X_1, X_2 \\overset{IID}{\\sim} F^*, \\,\\,X_3 \\overset{IID}{\\sim} G^*, F^* \\neq G^*\\end{array}$$ \n",
    "\n",
    "With the test statistic\n",
    "$$\\begin{array}{lcl}T(X_1,X_2,X_3) &=& \\text{abs} \\left(\\displaystyle\\frac{1}{2}\\displaystyle\\sum_{i=1}^2X_i - \\displaystyle\\frac{1}{1}\\displaystyle\\sum_{i=2+1}^3X_i\\right) \\\\ &=&\\text{abs}\\left(\\displaystyle\\frac{X_1+ X_2}{2} - \\displaystyle\\frac{X_3}{1}\\right)\\end{array}$$\n",
    "\n",
    "Our observed data $x_{obs} = (x_1, x_2, x_3) = (52, 54, 58)$\n",
    "\n",
    "and the realisation of the test statistic for this data is $t_{obs} = \\text{abs}\\left(\\displaystyle\\frac{52+54}{2} - \\frac{58}{1}\\right) = \\text{abs}\\left(53 - 58\\right) = \\text{abs}(-5) = 5$\n",
    "\n",
    "Now we need to tabulate the permutations and their probabilities.  There are 3! = 6 possible permutataions of three items.  For larger samples, you could use the `factorial` function to calculate this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factorial(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We said that under the null hypotheses (the samples have the same DF) each permutation is equally likely, so each permutation has probability $\\displaystyle\\frac{1}{6}$.\n",
    "\n",
    "There is a way in Python (the language under the hood in Sage), to get all the permuations of a sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[52, 54, 58],\n",
       " [52, 58, 54],\n",
       " [54, 52, 58],\n",
       " [54, 58, 52],\n",
       " [58, 52, 54],\n",
       " [58, 54, 52]]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(Permutations(totalSample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can tabulate the permuations, their probabilities, and the value of the test statistic that would be associated with that permutation:\n",
    "\n",
    "<table border=\"1\" cellpadding=\"5\" align=\"center\">\n",
    "<tbody>\n",
    "<tr>\n",
    "<td style=\"text-align: center;\">Permutation</td>\n",
    "<td style=\"text-align: center;\">$t$</td>\n",
    "<td style=\"text-align: center;\">$\\mathbf{P}_0(T=t)$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align: center;\"> </td>\n",
    "<td style=\"text-align: center;\"> </td>\n",
    "<td style=\"text-align: center;\">Probability under Null</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align: center;\">(52, 54, 58)</td>\n",
    "<td style=\"text-align: center;\">5</td>\n",
    "<td style=\"text-align: center;\">$\\frac{1}{6}$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align: center;\">(52, 58, 54)</td>\n",
    "<td style=\"text-align: center;\">&nbsp;1</td>\n",
    "<td style=\"text-align: center;\">$\\frac{1}{6}$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align: center;\">(54, 52, 58)</td>\n",
    "<td style=\"text-align: center;\">5</td>\n",
    "<td style=\"text-align: center;\">$\\frac{1}{6}$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align: center;\">(54, 58, 52)</td>\n",
    "<td style=\"text-align: center;\">4</td>\n",
    "<td style=\"text-align: center;\">$\\frac{1}{6}$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align: center;\">(58, 52, 54)</td>\n",
    "<td style=\"text-align: center;\">1</td>\n",
    "<td style=\"text-align: center;\">$\\frac{1}{6}$</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align: center;\">(58, 54, 52)</td>\n",
    "<td style=\"text-align: center;\">4</td>\n",
    "<td style=\"text-align: center;\">$\\frac{1}{6}$</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52, 54, 58]  has t =  5\n",
      "[52, 58, 54]  has t =  1\n",
      "[54, 52, 58]  has t =  5\n",
      "[54, 58, 52]  has t =  4\n",
      "[58, 52, 54]  has t =  1\n",
      "[58, 54, 52]  has t =  4\n"
     ]
    }
   ],
   "source": [
    "allPerms = list(Permutations(totalSample))\n",
    "for p in allPerms:\n",
    "    t = abs((p[0] + p[1])/2 - p[2]/1)\n",
    "    print (p, \" has t = \", t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "To calculate the P-value for our test statistic $t_{obs} = 5$, we need to look at how many permutations would give rise to test statistics that are at least as big, and add up their probabilities.\n",
    "\n",
    "$$\n",
    "\\begin{array}{lcl}\\text{P-value} &=& \\mathbf{P}_0(T \\geq t_{obs}) \\\\&=&\\mathbf{P}_0(T \\geq 5)\\\\&=&\\frac{1}{6} + \\frac {1}{6} \\\\&=&\\frac{2}{6}\\\\ &=&\\frac{1}{3} \\\\ &\\approx & 0.333\\end{array}\n",
    "$$\n",
    "\n",
    "We could write ourselves a little bit of code to do this in SageMath.  As you can see, we could easily improve this to make it more flexible so that we could use it for different numbers of samples, but it will do for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1/3"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allPerms = list(Permutations(totalSample))\n",
    "pProb = 1/len(allPerms)\n",
    "pValue = 0\n",
    "tobs = 5\n",
    "for p in allPerms:\n",
    "    t = abs((p[0] + p[1])/2 - p[2]/1)\n",
    "    if t >= tobs:\n",
    "        pValue = pValue + pProb\n",
    "pValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This means that there is little or no evidence against the null hypothesis (that the shell diameter observations are from the same DF).\n",
    "\n",
    "#### Pooled sample size\n",
    "\n",
    "The lowest possible P-value for a pooled sample of size $N=m+n$ is $\\displaystyle\\frac{1}{N!}$.  Can you see why this is? \n",
    "\n",
    "So with our small sub-samples the smallest possible P-value would be $\\frac{1}{6} \\approx 0.167$.  If we are looking for P-value $\\leq 0.01$ to constitute very strong evidence against $H_0$, then we have to have a large enough pooled sample for this to be possible.  Since $5! = 5 \\times 4 \\times 3 \\times 2 \\times 1 = 120$, it is good to have $N \\geq 5$\n",
    "\n",
    "#### YouTry in class\n",
    "\n",
    "Try copying and pasting our code and then adapting it to deal with a sub-sample (52, 54, 60) from the left of the pier and (58, 54) from the right side of the pier.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[52, 54, 60, 58, 54]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rightSub = [52, 54, 60]\n",
    "leftSub = [58, 54]\n",
    "totalSample = rightSub + leftSub\n",
    "totalSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### You will have to think about:\n",
    "\n",
    "- calculating the value of the test statistic for the observed data and for all the permuations of the total sample\n",
    "- calculating the probability of each permutation\n",
    "- calculating the P-value by adding the probabilities for the permutations with test statistics at least as large as the observed value of the test statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "(add more cells if you need them)\n",
    "\n",
    "(end of You Try)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can use the sample function and the Python method for making permutations to experiment with a larger sample, say 5 of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[60, 61, 49, 54, 59, 51, 55, 47, 60, 61]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n, m = 5, 5\n",
    "leftSub = sample(leftSide, n)\n",
    "rightSub = sample(rightSide,m)\n",
    "totalSample = leftSub + rightSub\n",
    "leftSub; rightSub; totalSample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9/5"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tobs = abs(mean(leftSub) - mean(rightSub))\n",
    "tobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We have met sample briefly already:  it is part of the Python random module and it does exactly what you would expect from the name: it samples a specified number of elements randomly from a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#define a helper function for calculating the tstat from a permutation\n",
    "def tForPerm(perm, samplesize1, samplesize2):\n",
    "    '''Calculates the t statistic for a permutation of data given the sample sizes to split the permuation into.\n",
    "    \n",
    "    Param perm is the permutation of data to be split into the two samples.\n",
    "    Param samplesize1, samplesize2 are the two sample sizes.\n",
    "    Returns the absolute value of the difference in the means of the two samples split out from perm.'''\n",
    "    sample1 = [perm[i] for i in range(samplesize1)]\n",
    "    sample2 = [perm[samplesize1+j] for j in range(samplesize2)]\n",
    "    return abs(mean(sample1) - mean(sample2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41/63"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allPerms = list(Permutations(totalSample))\n",
    "pProb = 1/len(allPerms)\n",
    "pValue = 0\n",
    "tobs = abs(mean(leftSub) - mean(rightSub))\n",
    "for p in allPerms:\n",
    "    t = tForPerm(p, n, m)\n",
    "    if t >= tobs:\n",
    "        pValue = pValue + pProb\n",
    "pValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n+m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3628800"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "factorial(n+m) # how many permutations is it checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "As you can see from the length of time it takes to do the calculation for $(5+5)! = 10!$ permutations, we will be here a long time if we try to this on all of the two shell data sets.  \n",
    "\n",
    "#### Monte Carlo methods to the rescue! \n",
    "We can use Monte Carlo integration to calculate an approximate P-value, and this will be our next topic. \n",
    "\n",
    " \n",
    "\n",
    "### You try\n",
    "\n",
    "Try working out the P-value for a sub-sample (58, 63) from the left of the pier and (61) from the right (the two last values in the left-side data set and the last value in the right-side one).  Do it as you would if given a similar question in the exam: you choose how much you want to use Sage to help and how much you do just with pen and paper. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Example: Permutation test for shell diameters using the bootsrapped/permuted confidence interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52, 54, 60, 60, 54, 47, 57, 58, 61, 57, 50, 60, 60, 60, 62, 44, 55, 58, 55, 60, 59, 65, 59, 63, 51, 61, 62, 61, 60, 61, 65, 43, 59, 58, 67, 56, 64, 47, 64, 60, 55, 58, 41, 53, 61, 60, 49, 48, 47, 42, 50, 58, 48, 59, 55, 59, 50, 47, 47, 33, 51, 61, 61, 52, 62, 64, 64, 47, 58, 58, 61, 50, 55, 47, 39, 59, 64, 63, 63, 62, 64, 61, 50, 62, 61, 65, 62, 66, 60, 59, 58, 58, 60, 59, 61, 55, 55, 62, 51, 61, 49, 52, 59, 60, 66, 50, 59, 64, 64, 62, 60, 65, 44, 58, 63]\n",
      "[58, 54, 60, 55, 56, 44, 60, 52, 57, 58, 61, 66, 56, 59, 49, 48, 69, 66, 49, 72, 49, 50, 59, 59, 59, 66, 62, 44, 49, 40, 59, 55, 61, 51, 62, 52, 63, 39, 63, 52, 62, 49, 48, 65, 68, 45, 63, 58, 55, 56, 55, 57, 34, 64, 66, 54, 65, 61, 56, 57, 59, 58, 62, 58, 40, 43, 62, 59, 64, 64, 65, 65, 59, 64, 63, 65, 62, 61, 47, 59, 63, 44, 43, 59, 67, 64, 60, 62, 64, 65, 59, 55, 38, 57, 61, 52, 61, 61, 60, 34, 62, 64, 58, 39, 63, 47, 55, 54, 48, 60, 55, 60, 65, 41, 61, 59, 65, 50, 54, 60, 48, 51, 68, 52, 51, 61, 57, 49, 51, 62, 63, 59, 62, 54, 59, 46, 64, 49, 61]\n",
      "115 139\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([52, 54, 60, 60, 54, 47, 57, 58, 61, 57, 50, 60, 60, 60, 62, 44, 55,\n",
       "       58, 55, 60, 59, 65, 59, 63, 51, 61, 62, 61, 60, 61, 65, 43, 59, 58,\n",
       "       67, 56, 64, 47, 64, 60, 55, 58, 41, 53, 61, 60, 49, 48, 47, 42, 50,\n",
       "       58, 48, 59, 55, 59, 50, 47, 47, 33, 51, 61, 61, 52, 62, 64, 64, 47,\n",
       "       58, 58, 61, 50, 55, 47, 39, 59, 64, 63, 63, 62, 64, 61, 50, 62, 61,\n",
       "       65, 62, 66, 60, 59, 58, 58, 60, 59, 61, 55, 55, 62, 51, 61, 49, 52,\n",
       "       59, 60, 66, 50, 59, 64, 64, 62, 60, 65, 44, 58, 63, 58, 54, 60, 55,\n",
       "       56, 44, 60, 52, 57, 58, 61, 66, 56, 59, 49, 48, 69, 66, 49, 72, 49,\n",
       "       50, 59, 59, 59, 66, 62, 44, 49, 40, 59, 55, 61, 51, 62, 52, 63, 39,\n",
       "       63, 52, 62, 49, 48, 65, 68, 45, 63, 58, 55, 56, 55, 57, 34, 64, 66,\n",
       "       54, 65, 61, 56, 57, 59, 58, 62, 58, 40, 43, 62, 59, 64, 64, 65, 65,\n",
       "       59, 64, 63, 65, 62, 61, 47, 59, 63, 44, 43, 59, 67, 64, 60, 62, 64,\n",
       "       65, 59, 55, 38, 57, 61, 52, 61, 61, 60, 34, 62, 64, 58, 39, 63, 47,\n",
       "       55, 54, 48, 60, 55, 60, 65, 41, 61, 59, 65, 50, 54, 60, 48, 51, 68,\n",
       "       52, 51, 61, 57, 49, 51, 62, 63, 59, 62, 54, 59, 46, 64, 49, 61])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (leftSide) # shell diameters on left side of pier\n",
    "print (rightSide) # on right side\n",
    "print (len(leftSide), len(rightSide))\n",
    "shellDiameterDataArray = np.array(leftSide + rightSide) # concatenate as np.array, observed data\n",
    "shellDiameterDataArray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[52 54 60 60 54 47 57 58 61 57 50 60 60 60 62 44 55 58 55 60 59 65 59 63\n",
      " 51 61 62 61 60 61 65 43 59 58 67 56 64 47 64 60 55 58 41 53 61 60 49 48\n",
      " 47 42 50 58 48 59 55 59 50 47 47 33 51 61 61 52 62 64 64 47 58 58 61 50\n",
      " 55 47 39 59 64 63 63 62 64 61 50 62 61 65 62 66 60 59 58 58 60 59 61 55\n",
      " 55 62 51 61 49 52 59 60 66 50 59 64 64 62 60 65 44 58 63]\n",
      "56.82608695652174\n",
      "[58 54 60 55 56 44 60 52 57 58 61 66 56 59 49 48 69 66 49 72 49 50 59 59\n",
      " 59 66 62 44 49 40 59 55 61 51 62 52 63 39 63 52 62 49 48 65 68 45 63 58\n",
      " 55 56 55 57 34 64 66 54 65 61 56 57 59 58 62 58 40 43 62 59 64 64 65 65\n",
      " 59 64 63 65 62 61 47 59 63 44 43 59 67 64 60 62 64 65 59 55 38 57 61 52\n",
      " 61 61 60 34 62 64 58 39 63 47 55 54 48 60 55 60 65 41 61 59 65 50 54 60\n",
      " 48 51 68 52 51 61 57 49 51 62 63 59 62 54 59 46 64 49 61]\n",
      "56.66187050359712\n"
     ]
    }
   ],
   "source": [
    "print (shellDiameterDataArray[0:115]) # left diameters\n",
    "print (shellDiameterDataArray[0:115].mean()) # mean of left side\n",
    "print (shellDiameterDataArray[115:115+139]) # right diameters\n",
    "print (shellDiameterDataArray[115:115+139].mean()) # mean of right side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16421645292462017"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observedTestStat = np.abs(shellDiameterDataArray[0:115].mean() - shellDiameterDataArray[115:115+139].mean())\n",
    "observedTestStat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16421645292462017"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use the right lamba expression for the statistic of interest\n",
    "TestStat = \\\n",
    "          lambda dataArray : np.abs(dataArray[0:115].mean() - dataArray[115:115+139].mean())\n",
    "observedTestStat = TestStat(shellDiameterDataArray)\n",
    "observedTestStat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([61, 52, 63, 45, 50, 52, 57, 56, 39, 57, 59, 55, 64, 57, 58, 65, 60,\n",
       "       65, 60, 54, 60, 49, 34, 48, 57, 59, 54, 54, 44, 50, 63, 58, 63, 65,\n",
       "       63, 63, 59, 49, 59, 58, 62, 46, 59, 55, 63, 61, 56, 59, 63, 44, 62,\n",
       "       65, 52, 64, 39, 59, 50, 62, 62, 55, 40, 62, 61, 60, 52, 62, 59, 64,\n",
       "       66, 42, 47, 65, 51, 59, 47, 59, 60, 60, 65, 61, 58, 60, 48, 58, 49,\n",
       "       58, 50, 58, 60, 67, 49, 50, 51, 52, 59, 65, 55, 60, 61, 44, 62, 64,\n",
       "       58, 59, 61, 48, 55, 43, 55, 61, 61, 58, 60, 50, 62, 56, 51, 60, 72,\n",
       "       54, 65, 61, 66, 34, 60, 57, 64, 47, 60, 44, 63, 54, 55, 62, 49, 65,\n",
       "       59, 58, 58, 44, 47, 60, 61, 61, 60, 54, 55, 62, 49, 56, 69, 65, 59,\n",
       "       61, 68, 64, 58, 52, 68, 39, 52, 59, 47, 55, 59, 61, 59, 63, 58, 67,\n",
       "       59, 58, 49, 61, 59, 60, 63, 66, 33, 59, 59, 51, 64, 48, 47, 59, 43,\n",
       "       64, 50, 55, 61, 63, 60, 55, 64, 62, 61, 60, 62, 58, 64, 56, 54, 62,\n",
       "       53, 61, 58, 62, 47, 51, 61, 50, 64, 61, 62, 57, 55, 55, 64, 62, 38,\n",
       "       62, 55, 62, 60, 61, 47, 64, 59, 43, 49, 64, 41, 57, 60, 61, 49, 65,\n",
       "       66, 48, 66, 64, 59, 51, 41, 47, 48, 52, 40, 64, 51, 61, 66, 65])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(sample(list(shellDiameterDataArray),115+139)) \n",
    "# resample from concatenated data to bootstrap TestStat under H_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observed Test Statistics =  0.16421645292462017\n",
      "The inner (1 - 0.0500000000000000 ) percentile based Confidence Interval for the statistic T = \n",
      "  [ 0.039061620269001375,1.7441601501407595 ]\n"
     ]
    }
   ],
   "source": [
    "alpha=0.05\n",
    "B=100 # number of bootstrap samples for estimating p-value in a permutation test / or bootstrapped as per 'sample'\n",
    "\n",
    "bootstrappedTestStats=[]\n",
    "for i in range(B):\n",
    "    # sample from concatenated data to bootstrap or permute as per 'sample(...)' function's behaviour\n",
    "    permutedBootstrappedData = np.array(sample(list(shellDiameterDataArray),115+139)) \n",
    "    #print i, permutedBootstrappedData\n",
    "    bootstrappedTestStats.append(TestStat(permutedBootstrappedData))\n",
    "    \n",
    "bootstrappedTestStatsArray=np.array(bootstrappedTestStats)\n",
    "\n",
    "print (\"observed Test Statistics = \", observedTestStat)\n",
    "#print bootstrappedTestStats\n",
    "\n",
    "alpaAsPercentage=alpha*100.0\n",
    "lowerBootstrap1MinusAlphaCIForStatisticT = np.percentile(bootstrappedTestStats,alpaAsPercentage/2)\n",
    "upperBootstrap1MinusAlphaCIForStatisticT = np.percentile(bootstrappedTestStats,100-alpaAsPercentage/2)\n",
    "\n",
    "print (\"The inner (1 - \"+str(alpha)+\" ) percentile based Confidence Interval for the statistic T = \")\n",
    "print (\"  [ \"+str(lowerBootstrap1MinusAlphaCIForStatisticT)+\",\" +str(upperBootstrap1MinusAlphaCIForStatisticT)+\" ]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### So do we reject the Null Hypothesis that the diameters were the same on either side of the New Brighton Pier?\n",
    "\n",
    "To answer this question we simply need to see whether the observed Test Statistic of the absolute difference between the sample means between the left and right side of the pier lies:\n",
    " - inside the 95% permuted/bootstrapped confidence interval?\n",
    " - or outside?\n",
    " \n",
    "We can conclude that there is no evidence to reject the null hypothesis because $0.164216452925 \\in [0.05, 0.82]$. And therefore the inviduals of *Dosinia anus* have the same distribution on either side of the Pier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath 9.1",
   "language": "sagemath",
   "metadata": {
    "cocalc": {
     "description": "Open-source mathematical software system",
     "priority": 1,
     "url": "https://www.sagemath.org/"
    }
   },
   "name": "sage-9.1",
   "resource_dir": "/ext/jupyter/kernels/sage-9.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "lx_course_instance": "2021",
  "lx_course_name": "Introduction to Data Science: A Comp-Math-Stat Approach",
  "lx_course_number": "1MS041"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
