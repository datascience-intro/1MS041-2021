{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2203951",
   "metadata": {
    "deletable": false
   },
   "source": [
    "# [Introduction to Data Science: A Comp-Math-Stat Approach](http://datascience-intro.github.io/1MS041-2021/)    \n",
    "## 1MS041, 2021 \n",
    "&copy;2021 Raazesh Sainudiin, Benny Avelin. [Attribution 4.0 International     (CC BY 4.0)](https://creativecommons.org/licenses/by/4.0/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Singular value decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In the book they explain the Singular Value Decomposition differently. The concept of singular value decomposition is tightly connected to Principal Component Analysis, and the methods to compute the SVD is how we get the components in PCA. We will therefore switch between explaining tthe ideas in the context of PCA as well as using the linear algebra \"geometric\" concepts. So, for those of you who know PCA but not SVD will have a hook, but also those of you who know linear algebra will understand the ideas of PCA.\n",
    "\n",
    "Consider a set of points in $R^d$, for instance the following plot (in $R^2$):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "ae99e84cbb4514ca433e4fdafb25a5672ea629ec",
      "text/plain": [
       "Graphics object consisting of 1 graphics primitive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.random.multivariate_normal(mean=(0,0),cov=[(1,0.8),(0.8,1)],size=(1000,))\n",
    "points(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "When we considered linear regression we considered linear approximations and minimized the squared error to find our function. That is, we only considered the y-component of our error. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Background\n",
    "\n",
    "### Projections of vectors\n",
    "\n",
    "Consider two vectors $a$ and $b$, we would like to project $a$ onto $b$, i.e.\n",
    "<img width=500px src=\"images/Projection_and_rejection.png\"></img>\n",
    "\n",
    "From the above picture we can think of the decomposition of $a$ in terms of $b$ as two vectors, namely $a_1$ and $a_2$, they are orthogonal, which we write as $a_1 \\perp a_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Lets say that we wish to approximate the data using a low-dimensional subspace, think of a low-dimensional hyperplane. In the case of 2d there is only 1d hyperplanes (lines), but if you have, say 100 dimensional we could consider the best fitting 10 dimensional hyperplane. What we mean with best fitting is that the distance from the point to its projection onto our subspace is as small as possible. Think of our 2d example above, then we would like to find the line such that orthogonal projection gives the smallest error. Just looking at the plot we would take the line $y=x$. \n",
    "\n",
    "<img src=\"images/first_singular_vector.png\"></img>\n",
    "\n",
    "But how do we formulate this rigorously?\n",
    "\n",
    "Consider a line given by the unit vector $v$, and consider a point $x$ then the projection of $x$ onto $v$ is as above given by\n",
    "$$\n",
    "    (v \\cdot x) v\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We will now use these ideas applied to IID samples of points $\\{X_1,\\ldots,X_n\\}\\in \\mathbb{R}^m$ with zero expectation. Let $w$ be a unit vector. Consider the projection of each $X_i$ onto $v$ but only consider the proportion i.e. $X_i \\cdot v$, then define\n",
    "$$\n",
    "    Y_i = (X_i \\cdot v)\n",
    "$$\n",
    "The line with maximal empirical variance can be written as\n",
    "$$\n",
    "    v_1:=\\arg\\max_{\\|v\\|=1} \\frac{1}{n} \\sum_i (Y_i - \\overline{Y}_n)^2 = \\arg\\max_{\\|v\\|=1} \\frac{1}{n} \\sum_{j=1}^n ((X_i \\cdot v) - \\overline{X \\cdot v}_n)^2 = \\arg\\max_{\\|v\\|=1} \\sum_{j=1}^n |X_i \\cdot v|^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "If we construct a matrix $A$ of size $n \\times m$ with rows $X_i$ then we can rewrite\n",
    "$$\n",
    "    \\sum_{j=1}^n |X_i \\cdot v|^2 = |Av|^2\n",
    "$$\n",
    "and our problem reduces to the linear algebra problem of given an $n \\times m$ matrix $A$ to find the direction that is most \"expanded/least contracted\" by $A$, in the following sense\n",
    "$$\n",
    "    \\arg\\max_{\\|w\\|=1} |Av|\n",
    "$$\n",
    "\n",
    "> Note, the singular vectors are not necessarily unique, in fact if $v$ is a singular vector, then so is $-v$. We can also have ties, in that case we arbitrarily pick one. In the following we will as in the book assume that the singular vectors can be picked uniquely, for instance by requiring no ties and that we fix the sign as to make the vector unique.\n",
    "\n",
    "### Definition\n",
    "> The vector $v_1 \\in \\mathbb{R}^m$ of the ($n \\times m$) matrix $A$, defined as\n",
    "> $$v_1:= \\arg\\max_{\\|v\\|=1} |Av|$$\n",
    "> is called the `first singular vector` of $A$.\n",
    "> The value $\\sigma_1(A)$ defined as\n",
    "> $$\\sigma_1(A) := |Av_1|$$\n",
    "> is called the `first singular value` of $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Note\n",
    "> In the context where $A$ is constructed from our IID vectors $X_1,\\ldots,X_n$, we see that $\\sigma_1$ is the standard deviation in the direction of the first singular vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now that we have defined the first singular vector, we can define the second singular vector. This is simply a vector that is orthogonal to $v_1$ again solving our maximum problem, i.e.\n",
    "$$\n",
    "    v_2 := \\arg\\max_{\\|v\\|=1, v \\perp v_1} |Av|\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can interpret this as follows, consider the plane given by the first singular vector $v_1$ as the normal, then we can consider our new problem by finding the vector $v$ that maximizes $|(P_1A)v|$ where $P_1A=\\{P_1A_{1,\\cdot},\\ldots,P_1A_{n,\\cdot}\\}$ where $P$ is the projection of a vector onto the plane $v_1 \\cdot x = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<img src=\"images/first_singular_vector.png\"></img><img src=\"images/projection_first_singular_vector.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Theorem (Greedy Algorithm)\n",
    "> Let $A$ be an n x d matrix with singular vectors $v_1,...,v_r$. For $1 \\leq k \\leq r$, let $V_k$ be the subspace spanned by $v_1,\\ldots,v_k$. For each $k$, $V_k$ is the best fit $k$-dimensional subspace for $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "What is $V_k$?\n",
    "$$\n",
    "    V_k = \\{\\alpha_1 v_1 + \\ldots + \\alpha_k v_k: (\\alpha_1,\\ldots,\\alpha_k) \\in \\mathbb{R}^k\\} =: span(\\{v_1,\\ldots,v_k\\}).\n",
    "$$\n",
    "What do we mean by best fit? Let $\\tilde V_k$ be another $k$-dimensional subspace consider the distance of a point $p$ to the $k$-dimensional subspace $\\tilde V_k$, such a space is spanned by an orthonormal basis $\\tilde v_1, \\ldots, \\tilde v_k$, the distance from $p$ to $\\tilde V_k$ can be seen to be\n",
    "$$\n",
    "    \\|p - proj_{\\tilde V_k} p\\| = \\|p - \\sum_{i=1}^k (\\tilde v_i \\cdot p)\\tilde v_i\\|\n",
    "$$\n",
    "We mean that  $V_k$  is  the $k$-dimensional subspace that minimizes\n",
    "$$\n",
    "    \\sum_{i=1}^n \\|X_i - proj_{\\tilde V_k} X_i\\|^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "But we can use the Pythagorean theorem to get\n",
    "$$\n",
    "    \\sum_{i=1}^n \\left (\\|proj_{\\tilde V_k}X_i\\|^2 +  \\|X_i - proj_{\\tilde V_k}X_i\\|^2 \\right ) = \\sum_{i=1}^n \\|X_i\\|^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "and thus we can get\n",
    "$$\n",
    "    \\sum_{i=1}^n \\left (\\|proj_{\\tilde V_k}X_i\\|^2 -  \\|X_i\\|^2 \\right ) = \\sum_{i=1}^n \\|X_i - proj_{\\tilde V_k}X_i\\|^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "From the above we see that the best fitting subspace is the subspace that maximizes the \"variance\" in the sense that we have seen. The point I am making is that we can rephrase the theorem as saying that finding $v_1,\\ldots,v_k$ in a greedy way by maximizing the variance is the same as directly minimizing the variance of the deviation from the subspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "def findFirstSingularVector(A):\n",
    "    '''Takes a matrix of form n x m and returns a tuple of the singular vector and singular value\n",
    "    '''\n",
    "    def negsumofsquares(v):\n",
    "        return -np.sum((A@v)**2)\n",
    "    constraints = ({'type': 'eq', 'fun': lambda w:  1-(np.sum(w**2))})\n",
    "    m = A.shape[1]\n",
    "    random_v = np.random.normal(size=m)\n",
    "    initial_vector=random_v/np.linalg.norm(random_v)\n",
    "    ans1 = minimize(negsumofsquares, x0=initial_vector,constraints=constraints)\n",
    "    if (ans1.success):\n",
    "        return ans1.x,np.sqrt(-ans1.fun)\n",
    "    else:\n",
    "        raise Exception(\"No convergence\")\n",
    "\n",
    "def projectOnPlane(A,normal):\n",
    "    '''Project the rows of A onto the plane given by \"normal * x = 0\"'''\n",
    "    \n",
    "    return A - (A@normal).reshape(-1,1)*normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can now use the code above to find the singular vectors, let us take a look at the example data I plotted above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.71191709 -0.70226352] 43.587923587503624\n"
     ]
    }
   ],
   "source": [
    "v1,s1 = findFirstSingularVector(X)\n",
    "print(v1,s1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "As we can see this is pretty much identical to the vector in the 45 degree direction. To find the second singular vector, we simply project our data onto the plane spanned by having $v_1$ as the normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "4aaa0a57b5dcd2c7209cfc982cd69c7080a4c927",
      "text/plain": [
       "Graphics object consisting of 1 graphics primitive"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_1A = projectOnPlane(X,v1)\n",
    "points(P_1A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.70226351 -0.7119171 ] 14.365618434588386\n"
     ]
    }
   ],
   "source": [
    "v_2,s2 = findFirstSingularVector(P_1A)\n",
    "print(v_2,s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Its clear which direction this is headed. Let us also look at what happens when we project the data onto the plane with normal $v_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "e84fa12572055fb51d7fe18e6c70475d9162ad1a",
      "text/plain": [
       "Graphics object consisting of 1 graphics primitive"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P=points(projectOnPlane(P_1A,v_2))\n",
    "P.xmax(1);P.xmin(-1);P.ymax(1);P.ymin(-1);P.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Question: What have we done, two projections in a row? What is the projection of a projection?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "It should be clear from the definition and the \"Greedy Algorithm\" theorem that $proj_{V_m} A = A$. That is, if we use all possible singular vectors, then we can represent the data from $A$ as points in $V_m$. That is any row of $A$ can be written as a linear combination of all the singular vectors.\n",
    "\n",
    "We can now implement `findSingularVectors` using our methods above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def findSingularVectors(A,k=1):\n",
    "    '''Takes a n x m matrix of points, where the points are on the rows. \n",
    "    It then computes the first k singular vectors with corresponding singular values'''\n",
    "    assert(len(A.shape) == 2), \"A needs to be a 2d matrix\"\n",
    "    assert((1 <= k) and (k <= A.shape[1])), \"We can only compute the number of singular vectors up to the dimension of the data\"\n",
    "    vectors = []\n",
    "    values = []\n",
    "    PA = A\n",
    "    for i in range(k):\n",
    "        v,s = findFirstSingularVector(PA)\n",
    "        PA = projectOnPlane(PA,v)\n",
    "        vectors.append(v)\n",
    "        values.append(s)\n",
    "    return vectors,values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "`findSingularVectors` on X is a simple as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([0.71191709, 0.70226352]), array([ 0.70226352, -0.71191709])],\n",
       " [43.5879235930413, 14.365618429059491])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findSingularVectors(X,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We can even do 3 dimension\n",
    "Z = np.random.normal(size=(1000,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([array([0.43379043, 0.06596745, 0.89859566]),\n",
       "  array([-0.07030653,  0.9967536 , -0.03923331]),\n",
       "  array([-0.89826655, -0.04615825,  0.43702016])],\n",
       " [32.59826037754821, 31.59420900435691, 29.414743263754758])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "findSingularVectors(Z,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Question: Is this a good way of finding the singular vectors?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**YouTry:**\n",
    "\n",
    "Test and see what happens if we try to apply our `findSingularValues` on something with more dimensions, like 10 dimension and we would like 5 singular vectors, what happens? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Singular Value Decomposition of a Matrix\n",
    "\n",
    "Remember that we said that if we compute $m$ singular vectors of the $n \\times m$ dimensional matrix $A$, then\n",
    "$$\n",
    "    proj_{V_m} A = A\n",
    "$$\n",
    "this implies that we can write each row in $A$ as $X_i = \\sum_{j=1}^m (X_i \\cdot v_j) v_j$ which we can now rewrite as\n",
    "$$\n",
    "    A = \\sum_{j=1}^m A v_j v_j^T\n",
    "$$\n",
    "denoting $u_i:=\\frac{A v_i}{\\sigma_i}$ we see that the above expression becomes\n",
    "$$\n",
    "    A = \\sum_{j=1}^m \\sigma_j u_j v_j^T\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This is the singular value decomposition. I.e. we have decomposed $A$ into a sum of matrices, that is $u_j v_j^T$ is $n \\times m$ matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Rewriting the above equation in matrix format we get\n",
    "$$\n",
    "    A = UDV^T\n",
    "$$\n",
    "where $U$ is the matrix with $u_1,\\ldots$ as the columns, $D$ is a diagonal matrix with $\\sigma_i$ as the diagonal and $V$ is the matrix with $v_i$ as columns of $V$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Definition**\n",
    "> The vectors $u_i$ are called the `left singular vectors`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Power method\n",
    "\n",
    "Now let us consider the matrix $A^TA$ this matrix has dimension $m \\times m$, using our decomposition above we obtain that\n",
    "$$\n",
    "    A^T A = (UDV^T)^T(UDV^T) = (VDU^T UDV^T) = VD^2V^T\n",
    "$$\n",
    "since $U^TU = I$ which comes from the fact that the columns are orthonormal. This means that for any column $v_i$ in $V$\n",
    "$$\n",
    "    A^T A v_i = VD^2V^T v_i = \\sigma_i^2 v_i\n",
    "$$\n",
    "so we see that $v_i$ is the $i$:th eigenvector of $A^T A$ with eigenvalue $\\sigma_i^2$.\n",
    "We can thus find the singular vectors by trying to find the eigenvectors of $A^T A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "How do we find the eigenvectors of $B = A^T A$? Well first note that\n",
    "$$\n",
    "    B^k = (V D^2 V^T)^k = (V D^{2k} V^T)\n",
    "$$\n",
    "by the same argument as above, i.e. $V^T V = I$. Thus we see that if $\\sigma_1 > \\sigma_2$ then if we let $k$ be large enough then\n",
    "$$\n",
    "    B^k \\approx (\\sigma_1)^{2k} v_1 v_1^T.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10)\n"
     ]
    }
   ],
   "source": [
    "Z1 = np.random.normal(size=(1000,10))\n",
    "Z1 = Z1-np.mean(Z1,axis=0).reshape(1,-1)\n",
    "B=Z1.T@Z1\n",
    "print(B.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "B_0 = B\n",
    "v1_approx = B[:,0]\n",
    "for i in range(200):\n",
    "    B_0 = B@B_0\n",
    "    B_0 = B_0 / np.linalg.norm(B_0) # Just normalize so that we reduce change of overflow\n",
    "    # Take the first column of B_0\n",
    "    b = B_0[:,0]\n",
    "    v1_approx = b/np.linalg.norm(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let us check what result we got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.17307599, -0.23962929, -0.3032961 , -0.46885279,  0.36048518,\n",
       "       -0.26414788,  0.04976714,  0.10971497, -0.25323512,  0.56784259])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1_approx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The vector we get is supposed to be an eigenvector of $B$, let us check if that is true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.21238273e-03, -2.67825079e-03,  1.54625490e-04,  1.47299257e-03,\n",
       "        2.93406625e-03,  2.78087820e-04,  2.59100001e-03,  3.69225009e-03,\n",
       "        3.53409382e-05, -3.11706391e-03])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bv1 = B@v1_approx\n",
    "v1_approx-Bv1/np.linalg.norm(Bv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Ok, so most components are of order $10^{-7}$, which is fairly accurate. Let us check with numpy's built in SVD and see what we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "U,D,VT = np.linalg.svd(Z1,full_matrices=False) # Interestingly enough numpy documentation calls this reduced SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "SVD from numpy returns $U,D,V.T$ so that we can write `Z1 = U@np.diag(D)@V.T`. Remember that the columns of $V$ is the singular vectors, so we can extract them as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.0178222 ,  0.04850188,  0.28080799,  0.51048594, -0.14074569,\n",
       "        0.25344231,  0.11589338,  0.13045542,  0.22865492, -0.70078927])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = VT.T\n",
    "V[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can also check if we get back our original matrix $Z1$ if we multiply everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.1398127522566523e-13"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# square error is of the order $10^{-13}$, pretty good\n",
    "np.linalg.norm(Z1-(U@np.diag(D)@VT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# PCA\n",
    "\n",
    "What is PCA, well basically it is a coordinate transformation from the original coordinates to the coordinate system given by the singular vectors. Since $V$ is orthonormal it is as simple as a product, i.e.\n",
    "\n",
    "$$\n",
    "    A = UDV^T\n",
    "$$\n",
    "Recall that each row in $A$ is a data point i.e. an $m$ dimensional vector and that $V$ is an orthonormal basis, as such we project each point in $A$ onto each basis vector from $V$ by using dot products, as in $(X_i \\cdot v_i)v_i$, the coordinate in the basis is just $X_i \\cdot v_i$, and as such we get\n",
    "$$\n",
    "    PCA(A) = AV =  UDV^TV = UD\n",
    "$$\n",
    "Lets try it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.76635149,  2.05570774, -0.13635384, ...,  0.04765146,\n",
       "        -0.87751416,  0.67527962],\n",
       "       [-0.38880805, -1.68711651, -0.43072077, ...,  0.55198908,\n",
       "         0.69774245, -0.4195591 ],\n",
       "       [ 0.88712757, -0.01433301, -0.73917784, ..., -1.49304488,\n",
       "        -1.36776005,  1.15753913],\n",
       "       ...,\n",
       "       [ 0.0503714 , -0.83830681,  0.64190555, ..., -0.08342697,\n",
       "        -0.90039476, -2.18728033],\n",
       "       [ 0.1947741 ,  0.39127624,  4.00866915, ...,  0.13284496,\n",
       "        -0.59507049, -0.47715572],\n",
       "       [ 1.49974953, -1.10664026, -1.69201254, ..., -0.5315721 ,\n",
       "         0.11957512, -0.4095345 ]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U@np.diag(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.13690769, -0.71214437, -0.13128913, ...,  0.06237733,\n",
       "        -0.9311444 , -0.60382278],\n",
       "       [-1.87079527, -0.56325569,  0.07489574, ..., -0.96202915,\n",
       "         0.80549259,  0.62332987],\n",
       "       [ 0.93739944, -1.53148254,  0.90713692, ..., -0.89808873,\n",
       "         0.9841879 , -0.39676187],\n",
       "       ...,\n",
       "       [ 1.33688098, -0.02959541, -0.37821044, ..., -1.45458732,\n",
       "        -0.44532597,  1.0763427 ],\n",
       "       [-0.39859239,  1.59748252, -2.16388461, ..., -0.25979978,\n",
       "        -2.37252184, -0.73021981],\n",
       "       [-0.327264  , -0.94891629,  1.90382775, ..., -1.63042026,\n",
       "         1.65318587, -0.69207775]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(Z1@V)@V.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##  PCA class\n",
    "\n",
    "since we dont all have `sklearn` lets implement our own PCA class that we can use from now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class PCA(object):\n",
    "\n",
    "    def __init__(self,n_components):\n",
    "        self.n_components =  n_components\n",
    "        self.components=None\n",
    "        self.singular_values=None\n",
    "        self.dimension=None\n",
    "        self.mean=None\n",
    "\n",
    "    def fit(self,X):\n",
    "        assert(type(X) == np.ndarray), \"Please input type np.ndarray!\"\n",
    "        assert(len(X.shape) == 2), \"Please input 2d array!\"\n",
    "        assert(min(X.shape[1],X.shape[0]) >= self.n_components), \"min(X.shape[1],X.shape[0]) >= self.n_components!\"\n",
    "        print(\"Number of data_points: %d, number of features: %d, Number of components: %d\" %(X.shape[0],X.shape[1],self.n_components))\n",
    "        \n",
    "        X = self._center(X) # Make sure the data is centered (each feature has empirical mean zero)\n",
    "        self.dimension=X.shape[1] # This is the dimension of the data\n",
    "        self.components,self.singular_values = self._compute(X) # Compute the singular vectors\n",
    "        \n",
    "    def _center(self,X):\n",
    "        self.mean = np.mean(X,axis=0).reshape(1,-1)\n",
    "        return X-self.mean\n",
    "        \n",
    "    def transform(self,X):\n",
    "        if (type(self.components) == None):\n",
    "            raise Exception(\"Not fitted yet!\")\n",
    "        \n",
    "        assert(type(X) == np.ndarray), \"Please input type np.ndarray!\"\n",
    "        assert(len(X.shape) == 2), \"Please input 2d array!\"\n",
    "        assert(X.shape[1] == self.dimension), \"Incorrect number of features!\"\n",
    "        \n",
    "        return (X-self.mean)@self.components\n",
    "    \n",
    "    def inverse_transform(self,X):\n",
    "        if (type(self.components) == None):\n",
    "            raise Exception(\"Not fitted yet!\")\n",
    "        \n",
    "        assert(type(X) == np.ndarray), \"Please input type np.ndarray!\"\n",
    "        assert(len(X.shape) == 2), \"Please input 2d array!\"\n",
    "        assert(X.shape[1] == self.n_components), \"Incorrect number of features!\"\n",
    "        \n",
    "        return X@self.components.T+self.mean\n",
    "    \n",
    "    def project(self,X):\n",
    "        return self.inverse_transform(self.transform(X))\n",
    "        \n",
    "    def _compute(self,X):\n",
    "        U,D,VT = np.linalg.svd(X,full_matrices=False)\n",
    "        return (VT.T)[:,:self.n_components],D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = PCA(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data_points: 1000, number of features: 10, Number of components: 2\n"
     ]
    }
   ],
   "source": [
    "pca.fit(Z1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# SVD in Action\n",
    "\n",
    "This is all cool and such, but what can you do with it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Singular value decomposition can be used in the following ways\n",
    "\n",
    "## Factor Analysis\n",
    "\n",
    "* Studying underlying factors. The famous `g factor`: proposed by Spearman (Spearman correlation), to describe \"general intelligence\" as a singular vector based on data about IQ, Math ability and other cognitive tests. This is also called Factor analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"95%\"\n",
       "            height=\"300\"\n",
       "            src=\"https://en.wikipedia.org/wiki/G_factor_(psychometrics)#Factor_structure_of_cognitive_abilities\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame object at 0x2b152a7d0>"
      ]
     },
     "execution_count": 611,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def showURL(url, ht=500):\n",
    "    \"\"\"Return an IFrame of the url to show in notebook with height ht\"\"\"\n",
    "    from IPython.display import IFrame\n",
    "    return IFrame(url, width='95%', height=ht) \n",
    "showURL('https://en.wikipedia.org/wiki/G_factor_(psychometrics)#Factor_structure_of_cognitive_abilities',300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"95%\"\n",
       "            height=\"500\"\n",
       "            src=\"https://en.wikipedia.org/wiki/Principal_component_analysis#Applications\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame object at 0x2b846b850>"
      ]
     },
     "execution_count": 612,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showURL('https://en.wikipedia.org/wiki/Principal_component_analysis#Applications')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "* Compressing a representation of data, as a dimensional reduction technique. This is similar to the rank $k$ approximation idea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Example on compressing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_digits():\n",
    "    import csv\n",
    "    data = []\n",
    "    target = []\n",
    "    with open('data/digits.csv', mode='r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        header = next(reader)\n",
    "        for row in reader:\n",
    "            data.append([float(x) for x in row[:-1]])\n",
    "            target.append(int(row[-1]))\n",
    "    return np.array(data),np.array(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X,Y = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "ded3654312be8dfe10d50b1a45dfbf216ffcc802",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "execution_count": 615,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(2,5)\n",
    "plt.gray()\n",
    "for i in range(10):\n",
    "    row = floor(i/5)\n",
    "    column = i % 5\n",
    "    ax[row,column].imshow(X[i,:].reshape(8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca_digit = PCA(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data_points: 1797, number of features: 64, Number of components: 10\n"
     ]
    }
   ],
   "source": [
    "pca_digit.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "9f001ca46d5eef0eeeaf1fd18993b99aee90942e",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "execution_count": 618,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_projected = pca_digit.project(X)\n",
    "fig, ax = plt.subplots(2,5)\n",
    "plt.gray()\n",
    "for i in range(10):\n",
    "    row = floor(i/5)\n",
    "    column = i % 5\n",
    "    ax[row,column].imshow(X_projected[i,:].reshape(8,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "What we can see is that even with only 10 components we were able to fairly well represent the digits, although it is clear that some are not so easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Explained variance\n",
    "\n",
    "Explained variance is how much percentage of the total variance is captured by our singular vectors. Remember the interpretation of the singular values as the standard deviation, as such the variance explained of the first $k$ components is just the sum of the singular values squared and divided by the total variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "variances = pca_digit.singular_values**2/(X.shape[0]-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.14890594, 0.28509365, 0.40303959, 0.48713938, 0.54496353,\n",
       "       0.59413263, 0.6372925 , 0.67390623, 0.70743871, 0.73822677,\n",
       "       0.76195018, 0.78467714, 0.80289578, 0.82063433, 0.83530534,\n",
       "       0.84940249, 0.86258838, 0.87506976, 0.88524694, 0.89430312,\n",
       "       0.9031985 , 0.91116973, 0.91884467, 0.9260737 , 0.93303259,\n",
       "       0.9389934 , 0.94474955, 0.94990113, 0.95479652, 0.9590854 ,\n",
       "       0.96282146, 0.96635421, 0.96972105, 0.97300135, 0.97608455,\n",
       "       0.97902234, 0.98158823, 0.98386565, 0.98608843, 0.98820273,\n",
       "       0.99010182, 0.99168835, 0.99319995, 0.99460574, 0.99577196,\n",
       "       0.99684689, 0.99781094, 0.99858557, 0.99914278, 0.99954711,\n",
       "       0.99975703, 0.99983951, 0.99989203, 0.99994255, 0.99997555,\n",
       "       0.99998798, 0.99999503, 0.99999804, 0.99999911, 0.99999966,\n",
       "       1.        , 1.        , 1.        , 1.        ])"
      ]
     },
     "execution_count": 620,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cumsum(variances/np.sum(variances))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Decorrelation\n",
    "Note that since the singular vectors are orthogonal, we immediately have that the transformed values always have zero correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Another example on compressing data, building a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "features = []\n",
    "labels = []\n",
    "with open('data/leukemia.csv',mode='r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    header=next(f)\n",
    "    for row in reader:\n",
    "        features.append(np.array(row[:-1],dtype=float))\n",
    "        labels.append((row[-1] == 'ALL')*1)\n",
    "\n",
    "X = np.stack(features,axis=0)\n",
    "Y = np.array(labels)\n",
    "\n",
    "# https://github.com/mavaladezt/kNN-from-Scratch\n",
    "\n",
    "def knn_distances(xTrain,xTest,k):\n",
    "    \"\"\"\n",
    "    Finds the k nearest neighbors of xTest in xTrain.\n",
    "    Input:\n",
    "    xTrain = n x d matrix. n=rows and d=features\n",
    "    xTest = m x d matrix. m=rows and d=features (same amount of features as xTrain)\n",
    "    k = number of nearest neighbors to be found\n",
    "    Output:\n",
    "    dists = distances between all xTrain and all XTest points. Size of n x m\n",
    "    indices = k x m matrix with the indices of the yTrain labels that represent the point\n",
    "    \"\"\"\n",
    "    #the following formula calculates the Euclidean distances.\n",
    "    import numpy as np\n",
    "    distances = -2 * xTrain@xTest.T + np.sum(xTest**2,axis=1) + np.sum(xTrain**2,axis=1)[:, np.newaxis]\n",
    "    #because of float precision, some small numbers can become negatives. Need to be replace with 0.\n",
    "    distances[distances < 0] = 0\n",
    "    distances = distances**.5\n",
    "    indices = np.argsort(distances, 0) #get indices of sorted items\n",
    "    distances = np.sort(distances,0) #distances sorted in axis 0\n",
    "    #returning the top-k closest distances.\n",
    "    return indices[0:k,:], distances[0:k,:]\n",
    "\n",
    "def knn_predictions(xTrain,yTrain,xTest=None,k=3):\n",
    "    \"\"\"\n",
    "    Uses xTrain and yTrain to predict xTest.\n",
    "    Input:\n",
    "    xTrain = n x d matrix. n=rows and d=features\n",
    "    yTrain = n x 1 array. n=rows with label value\n",
    "    xTest = m x d matrix. m=rows and d=features (same amount of features as xTrain)\n",
    "    k = number of nearest neighbors to be found\n",
    "    Output:\n",
    "    predictions = predicted labels, ie preds(i) is the predicted label of xTest(i,:)\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    if (xTest == None):\n",
    "        xTest = xTrain\n",
    "        \n",
    "    indices, distances = knn_distances(xTrain,xTest,k)\n",
    "    yTrain = yTrain.flatten()\n",
    "    rows, columns = indices.shape\n",
    "    predictions = list()\n",
    "    for j in range(columns):\n",
    "        temp = list()\n",
    "        for i in range(rows):\n",
    "            cell = indices[i][j]\n",
    "            temp.append(yTrain[cell])\n",
    "        predictions.append(max(temp,key=temp.count)) #this is the key function, brings the mode value\n",
    "    predictions=np.array(predictions)\n",
    "    return predictions\n",
    "\n",
    "def score(prediction,true_values):\n",
    "    return np.sum(prediction == true_values)/len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9583333333333334"
      ]
     },
     "execution_count": 622,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(knn_predictions(X,Y,k=5),Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data_points: 72, number of features: 7129, Number of components: 2\n"
     ]
    }
   ],
   "source": [
    "k = 2\n",
    "pca_leukemia = PCA(n_components=k)\n",
    "pca_leukemia.fit(X)\n",
    "X_leukemia_project = pca_leukemia.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9583333333333334"
      ]
     },
     "execution_count": 624,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score(knn_predictions(X_leukemia_project,Y,k=5),Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "b237676b14f97a2817909cf99e726749033bfe7a",
      "text/plain": [
       "Graphics object consisting of 2 graphics primitives"
      ]
     },
     "execution_count": 625,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def standardScaler(X_in):\n",
    "    '''Takes an array of shape (n_samples,n_features) and centers and normalizes the data'''\n",
    "    X_out = (X_in-np.mean(X_in,axis=0))/np.std(X_in,axis=0)\n",
    "    return X_out\n",
    "\n",
    "if (k in [2,3]):\n",
    "    X_leukemia_project_rescale = standardScaler(X_leukemia_project)\n",
    "    class0 = X_leukemia_project_rescale[Y==0]\n",
    "    class1 = X_leukemia_project_rescale[Y==1]\n",
    "    P=points(class0,color='blue',size=20)\n",
    "    P+=points(class1,color='red',size=20)\n",
    "    P.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "From the above plot, it should be possible to use logistic regression to solve this problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data_points: 72, number of features: 7129, Number of components: 2\n"
     ]
    }
   ],
   "source": [
    "pca_leukemia_2d = PCA(n_components=2)\n",
    "pca_leukemia_2d.fit(X)\n",
    "X_leukemia_project_2d = standardScaler(pca_leukemia_2d.transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Cool, lets create a class to do LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(object):\n",
    "    def __init__(self,lam=0,max_iter=10000):\n",
    "        self.coeffs = None\n",
    "        self.result = None\n",
    "        self.lam = lam\n",
    "        self.max_iter = max_iter\n",
    "    \n",
    "    def fit(self,X,Y):\n",
    "        import numpy as np\n",
    "        from scipy import optimize\n",
    "\n",
    "        # define the objective/cost/loss function we want to minimise\n",
    "        def f(x):\n",
    "            return np.sum(np.log(1+np.exp(-(2*Y-1)*(x[0] + (np.dot(X,x[1:]))))))+self.lam*np.sum(x**2)\n",
    "\n",
    "        # multi-dimensional optimisation is syntactically similar to 1D, \n",
    "        # but we are using Gradient and Hessian information from numerical evaluation of f to \n",
    "        # iteratively improve the solution along the steepest direction, etc. \n",
    "        # It 'LBFGS' method you will see in scientific computing\n",
    "        b = [-100,100]\n",
    "        parameter_bounding_box=([b]*(X.shape[1]+1)) # specify the constraints for each parameter\n",
    "        initial_arguments = np.array([0]*(X.shape[1]+1)) # point in 2D to initialise the minimize algorithm\n",
    "        result = optimize.minimize(f, initial_arguments, bounds=parameter_bounding_box,options={'maxiter':self.max_iter}) # just call the minimize method!\n",
    "        self.result = result\n",
    "        assert result.success, \"Not converged\"\n",
    "        \n",
    "        self.coeffs = result.x\n",
    "        \n",
    "        \n",
    "    \n",
    "    def predict(self,X):\n",
    "        if (self.coeffs is not None):\n",
    "            theta = self.coeffs[0]+(X@(self.coeffs[1:]).reshape(-1,1))\n",
    "            prediction = ((1/(1+exp(-theta)))>1/2)*1\n",
    "            return prediction.ravel()\n",
    "    \n",
    "    def score(self,X,Y):\n",
    "        return np.mean(Y==self.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(lam=0)\n",
    "lr.fit(X_leukemia_project_2d,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9444444444444444"
      ]
     },
     "execution_count": 629,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_leukemia_project_2d,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Ok, so what happens if we try our code on the full data-set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/avelin/opt/miniconda3/envs/sage/lib/python3.7/site-packages/sage/repl/ipython_kernel/__main__.py:14: RuntimeWarning: overflow encountered in exp\n",
      "/Users/avelin/opt/miniconda3/envs/sage/lib/python3.7/site-packages/scipy/optimize/_numdiff.py:497: RuntimeWarning: invalid value encountered in subtract\n",
      "  df = fun(x) - f0\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Not converged",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-630-ed074dc5e983>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_sc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlr_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_sc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-627-8ff6416882e9>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_arguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparameter_bounding_box\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'maxiter'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# just call the minimize method!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Not converged\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoeffs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Not converged"
     ]
    }
   ],
   "source": [
    "lr_full = LogisticRegression()\n",
    "\n",
    "X_sc = standardScaler(X)\n",
    "\n",
    "lr_full.fit(X_sc,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "OK, so thats a problem, why? Basically it is because the log-likelihood function is not numerically stable in the sense that even for moderate values $e^x$ is outside the computable range. There are ways around this using approximations, because remember that $\\log(1+e^{-x})$ is actually not a superbad function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "2ac949d18e7afde58dd63de2bbb7402be302ee1c",
      "text/plain": [
       "Graphics object consisting of 1 graphics primitive"
      ]
     },
     "execution_count": 631,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot(log(1+exp(-x)),x,-10,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The implementation in `sklearn` solves much of this and it allows us to complete our model fitting on the full dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Comments\n",
    "\n",
    "You would think that since both PCA and Logistic regression are linear that PCA+LogisticRegression would be the same as just LogisticRegression in the first place. This is not true, as PCA and LogisticRegression are minimizing different things, remember, for instance, the way we measure error (vertical / orthogonal).\n",
    "\n",
    "We can also illustrate it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data_points: 72, number of features: 7129, Number of components: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9444444444444444"
      ]
     },
     "execution_count": 632,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets try to map it to 2d\n",
    "pca1 = PCA(n_components=2)\n",
    "pca1.fit(X)\n",
    "lr1 = LogisticRegression()\n",
    "lr1.fit(standardScaler(pca1.transform(X)),Y)\n",
    "lr1.score(standardScaler(pca1.transform(X)),Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data_points: 72, number of features: 7129, Number of components: 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9444444444444444"
      ]
     },
     "execution_count": 633,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets try to map it to 5d\n",
    "pca2 = PCA(n_components=5)\n",
    "pca2.fit(X)\n",
    "lr2 = LogisticRegression()\n",
    "lr2.fit(standardScaler(pca2.transform(X)),Y)\n",
    "lr2.score(standardScaler(pca2.transform(X)),Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data_points: 72, number of features: 7129, Number of components: 10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 634,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets try to map it to 10d\n",
    "pca3 = PCA(n_components=10)\n",
    "pca3.fit(X)\n",
    "lr3 = LogisticRegression()\n",
    "lr3.fit(standardScaler(pca3.transform(X)),Y)\n",
    "lr3.score(standardScaler(pca3.transform(X)),Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Intriguing, so we see that it is something else. Perhaps, at this point you might also remember that PCA does not use the labels, it just represents the data $X$ in different coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Scaling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class PCAScaler(PCA):\n",
    "\n",
    "    def __init__(self,n_components=1):\n",
    "        PCA.__init__(self,n_components)\n",
    "    \n",
    "    def transform(self,X):\n",
    "        if (type(self.components) == None):\n",
    "            raise Exception(\"Not fitted yet!\")\n",
    "        \n",
    "        assert(type(X) == np.ndarray), \"Please input type np.ndarray!\"\n",
    "        assert(len(X.shape) == 2), \"Please input 2d array!\"\n",
    "        assert(X.shape[1] == self.dimension), \"Incorrect number of features!\"\n",
    "        \n",
    "        X_trans = PCA.transform(self,X)\n",
    "        X_rescaled = X_trans/self.singular_values\n",
    "        \n",
    "        return X_rescaled\n",
    "    \n",
    "    def inverse_transform(self,X):\n",
    "        X_out = PCA.inverse_transform(self,X)-self.mean\n",
    "        \n",
    "        return X_out\n",
    "    \n",
    "    def fit_transform(self,X):\n",
    "        self.fit(X)\n",
    "        return self.transform(X)\n",
    "    \n",
    "    def fit_transform_inverse_transform(self,X):\n",
    "        return self.inverse_transform(self.fit_transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data_points: 72, number of features: 7129, Number of components: 72\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-4.65618594e-02, -8.06560817e-02,  1.16142890e-01, ...,\n",
       "         1.98777979e-01, -8.04499931e-02, -1.82102223e-01],\n",
       "       [-1.41386155e-01,  2.81507786e-04, -4.20989991e-04, ...,\n",
       "        -1.77340276e-03, -1.10313769e-02, -1.20437978e-01],\n",
       "       [-7.19480180e-02, -8.90149307e-02, -1.44936289e-01, ...,\n",
       "        -2.71308720e-03, -6.19764331e-02, -3.75766492e-02],\n",
       "       ...,\n",
       "       [ 1.15490338e-01, -9.30875178e-02, -3.15082032e-02, ...,\n",
       "         2.62603826e-02,  1.50688252e-01, -1.15620459e-02],\n",
       "       [-3.35440720e-02, -5.09992249e-02,  6.89532582e-02, ...,\n",
       "        -1.93112875e-01, -1.99104434e-01,  6.16642448e-02],\n",
       "       [ 8.74890480e-03, -7.08617052e-02,  1.25388253e-01, ...,\n",
       "         2.09269403e-01,  2.83971544e-01, -4.81751912e-03]])"
      ]
     },
     "execution_count": 644,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcaScaler = PCAScaler(72)\n",
    "pcaScaler.fit(X)\n",
    "pcaScaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data_points: 72, number of features: 7129, Number of components: 72\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 645,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_pca = LogisticRegression()\n",
    "pcaScaler2 = PCAScaler(2)\n",
    "X_transformed_and_scaled = pcaScaler.fit_transform(X)\n",
    "lr_pca.fit(X_transformed_and_scaled,Y)\n",
    "lr_pca.score(X_transformed_and_scaled,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "The PCA scaler does a better job than the standard scaler in this case. In fact it is more intune with the shape of the data and as such it makes more sense. This is a key tool that you can use in your life as a data scientist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Recommender engines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let us say that we have a library of movies, lets say that we have $m$ movies. Furthermore assume that we have $n$ users that are watching these movies and outputting a like whenever they liked a movie. We can represent this as a matrix of the form $n \\times m$, with a $1$ in position $i,j$ if user $i$ liked movie $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "There is an idea that is called collaborative filtering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"95%\"\n",
       "            height=\"500\"\n",
       "            src=\"https://en.wikipedia.org/wiki/Collaborative_filtering\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame object at 0x2a2c21910>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showURL('https://en.wikipedia.org/wiki/Collaborative_filtering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"95%\"\n",
       "            height=\"500\"\n",
       "            src=\"https://en.wikipedia.org/wiki/Matrix_factorization_(recommender_systems)\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame object at 0x2a2bd1410>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showURL('https://en.wikipedia.org/wiki/Matrix_factorization_(recommender_systems)',500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Note that we can from the singular value decomposition and what we did with PCA we can do the following\n",
    "\n",
    "$$\n",
    "    A = U \\Sigma V^T\n",
    "    = \\begin{bmatrix} U_1 & U_2 \\end{bmatrix} \\begin{bmatrix} \\Sigma_r & 0 \\\\ 0 & 0 \\end{bmatrix} \\begin{bmatrix} V_1^T \\\\ V_2^T \\end{bmatrix}\n",
    "    = U_1 \\left(\\Sigma_r V_1^T\\right) .\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This method is a type of ranking factorization, note that for $C = U_1$ and $F = \\Sigma_r V_1^T$, then the product $CF$ is a low rank factorization approximation of $A$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['userId', 'movieId', 'rating', 'timestamp']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "user_item_data=[]\n",
    "with open('data/ratings.csv',mode='r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    print(next(reader))\n",
    "    for row in reader:\n",
    "        user_item_data.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "movie_ids = [row[1] for row in user_item_data]\n",
    "users = [row[0] for row in user_item_data]\n",
    "#user_item_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_movies = len(set(movie_ids))\n",
    "movie_to_index_dict = dict(zip(set(movie_ids),range(n_movies)))\n",
    "n_users = len(set(users))\n",
    "user_to_index_dict = dict(zip(set(users),range(n_users)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_item = np.zeros((n_users,n_movies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for row in user_item_data:\n",
    "    user_index = user_to_index_dict[row[0]]\n",
    "    movie_index = movie_to_index_dict[row[1]]\n",
    "    if (float(row[2]) > 3): # Lets say that rating >3 is that user liked it\n",
    "        user_item[user_index,movie_index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "U,D,VT = np.linalg.svd(user_item,full_matrices=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Lets choose a small value for the rank, lets say $100$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_latent = 100\n",
    "C = U[:,:n_latent]\n",
    "F = np.diag(D[:n_latent])@VT[:n_latent,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(610, 100)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 9724)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3111024693758507"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.abs(user_item-(C@F > 1/2)))/np.sum(user_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now the idea is that $CF$ is the prediction of the scores, and we would construct a recommendation for each user based on that which is predicted in $CF$ but the user has not yet seen. We could set a threshold for predicted as $>1/2$ or we could simply list the recommendations in order of descending predicted score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This is a simple recommender system and this is a fairly large field of study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SageMath 9.1",
   "language": "sagemath",
   "metadata": {
    "cocalc": {
     "description": "Open-source mathematical software system",
     "priority": 1,
     "url": "https://www.sagemath.org/"
    }
   },
   "name": "sage-9.1",
   "resource_dir": "/ext/jupyter/kernels/sage-9.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "lx_course_instance": "2021",
  "lx_course_name": "Introduction to Data Science: A Comp-Math-Stat Approach",
  "lx_course_number": "1MS041"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
